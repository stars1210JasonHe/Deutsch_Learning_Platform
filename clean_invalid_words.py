#!/usr/bin/env python3
"""
æ¸…ç†æ— æ•ˆè¯æ¡
1. æ£€æµ‹è‹±è¯­è¯æ¡å¹¶åˆ é™¤
2. æ£€æµ‹å¤æ•°å½¢å¼çš„ä¸»è¯æ¡ï¼Œé‡æ–°æŒ‡å‘å•æ•°
3. æ£€æµ‹æ— æ•ˆ/é”™è¯¯è¯æ¡
ä½¿ç”¨OpenAIæ¥åˆ¤æ–­è¯æ¡çš„æœ‰æ•ˆæ€§
"""
import sqlite3
import asyncio
import json
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.getcwd())

try:
    from app.services.openai_service import OpenAIService
    from app.core.config import settings
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥åº”ç”¨æ¨¡å—: {e}")
    sys.exit(1)

class WordCleaner:
    
    def __init__(self, batch_size=50, delay=1.5):
        self.db_path = 'data/app.db'
        self.openai_service = OpenAIService()
        self.batch_size = batch_size
        self.delay = delay
        self.stats = {
            'words_analyzed': 0,
            'english_words_deleted': 0,
            'plurals_redirected': 0,
            'invalid_words_deleted': 0,
            'corrections_made': 0,
            'start_time': datetime.now()
        }
    
    async def analyze_word_validity(self, words_batch):
        """ä½¿ç”¨OpenAIåˆ†æä¸€æ‰¹è¯æ¡çš„æœ‰æ•ˆæ€§"""
        try:
            words_list = [{"id": w[0], "lemma": w[1], "pos": w[2]} for w in words_batch]
            
            prompt = f"""
åˆ†æä»¥ä¸‹å¾·è¯­è¯æ¡çš„æœ‰æ•ˆæ€§ã€‚å¯¹æ¯ä¸ªè¯æ¡ï¼Œåˆ¤æ–­ï¼š
1. æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å¾·è¯­è¯æ±‡
2. æ˜¯å¦æ˜¯è‹±è¯­è¯æ±‡ï¼ˆåº”è¯¥åˆ é™¤ï¼‰
3. æ˜¯å¦æ˜¯å¤æ•°å½¢å¼ï¼ˆåº”è¯¥é‡å®šå‘åˆ°å•æ•°ï¼‰
4. æ­£ç¡®çš„è¯æ€§å’ŒåŸºç¡€å½¢å¼

è¯æ¡åˆ—è¡¨: {json.dumps(words_list, ensure_ascii=False)}

è¿”å›JSONæ ¼å¼:
{{
    "analyses": [
        {{
            "id": è¯æ¡ID,
            "lemma": "åŸè¯æ¡",
            "status": "valid|english|plural|invalid|correct_lemma",
            "reason": "åˆ¤æ–­ç†ç”±",
            "suggestion": {{
                "action": "keep|delete|redirect|correct",
                "target_lemma": "å¦‚æœéœ€è¦é‡å®šå‘ï¼Œç›®æ ‡è¯æ¡",
                "correct_pos": "æ­£ç¡®çš„è¯æ€§",
                "notes": "é¢å¤–è¯´æ˜"
            }}
        }}
    ]
}}

åˆ¤æ–­æ ‡å‡†ï¼š
- "english": è¿™æ˜¯è‹±è¯­è¯æ±‡ï¼Œä¸æ˜¯å¾·è¯­
- "plural": è¿™æ˜¯å¾·è¯­å¤æ•°å½¢å¼ï¼Œåº”é‡å®šå‘åˆ°å•æ•°
- "invalid": ä¸æ˜¯æœ‰æ•ˆè¯æ±‡æˆ–ä¸¥é‡é”™è¯¯
- "correct_lemma": è¯æ¡æ­£ç¡®ä½†è¯æ€§å¯èƒ½éœ€è¦ä¿®æ­£
- "valid": è¯æ¡å®Œå…¨æ­£ç¡®

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
            
            response = await self.openai_service.client.chat.completions.create(
                model=self.openai_service.model,
                messages=[
                    {"role": "system", "content": "You are a German language expert. Analyze word validity precisely and return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                max_tokens=2000,
                temperature=0.3
            )
            
            if response and response.choices:
                content = response.choices[0].message.content.strip()
                data = json.loads(content)
                return data.get('analyses', [])
            
            return []
            
        except Exception as e:
            print(f"   âŒ OpenAIåˆ†æå¤±è´¥: {e}")
            return []
    
    def get_suspicious_words(self, limit=100):
        """è·å–å¯ç–‘çš„è¯æ¡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æŸ¥æ‰¾å¯èƒ½æœ‰é—®é¢˜çš„è¯æ¡
            cursor.execute("""
                SELECT id, lemma, pos, cefr, frequency, notes
                FROM word_lemmas 
                WHERE 
                    -- è‹±è¯­è¯æ±‡æ¨¡å¼
                    lemma REGEXP '^[a-zA-Z]+ing$'  -- -ingç»“å°¾
                    OR lemma REGEXP '^[a-zA-Z]+ed$'   -- -edç»“å°¾
                    OR lemma REGEXP '^[a-zA-Z]+tion$' -- -tionç»“å°¾
                    OR lemma REGEXP '^[a-zA-Z]+ly$'   -- -lyç»“å°¾
                    -- å¯èƒ½çš„å¤æ•°å½¢å¼
                    OR (pos = 'noun' AND lemma LIKE '%en' AND LENGTH(lemma) > 4)
                    OR (pos = 'noun' AND lemma LIKE '%er' AND LENGTH(lemma) > 4) 
                    OR (pos = 'noun' AND lemma LIKE '%e' AND LENGTH(lemma) > 3)
                    -- å¯ç–‘çš„è¯
                    OR lemma IN ('the', 'and', 'or', 'but', 'with', 'for', 'to', 'of', 'in', 'on', 'at')
                    -- æ²¡æœ‰CEFRçº§åˆ«çš„å¯ç–‘è¯
                    OR (cefr IS NULL AND frequency IS NULL AND LENGTH(lemma) < 3)
                ORDER BY 
                    CASE WHEN cefr IS NULL THEN 1 ELSE 0 END,
                    LENGTH(lemma),
                    lemma
                LIMIT ?
            """, (limit,))
            
            results = cursor.fetchall()
            print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªå¯ç–‘è¯æ¡è¿›è¡Œåˆ†æ")
            return results
            
        except sqlite3.OperationalError:
            # SQLiteå¯èƒ½ä¸æ”¯æŒREGEXPï¼Œä½¿ç”¨ç®€åŒ–æŸ¥è¯¢
            cursor.execute("""
                SELECT id, lemma, pos, cefr, frequency, notes
                FROM word_lemmas 
                WHERE 
                    lemma GLOB '*ing'
                    OR lemma GLOB '*ed'
                    OR lemma GLOB '*tion'
                    OR lemma GLOB '*ly'
                    OR lemma IN ('the', 'and', 'or', 'but', 'with', 'for', 'to', 'of', 'in', 'on', 'at')
                    OR (cefr IS NULL AND frequency IS NULL AND LENGTH(lemma) < 3)
                ORDER BY LENGTH(lemma), lemma
                LIMIT ?
            """, (limit,))
            
            results = cursor.fetchall()
            print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªå¯ç–‘è¯æ¡è¿›è¡Œåˆ†æ")
            return results
            
        finally:
            conn.close()
    
    def get_random_words_for_check(self, limit=50):
        """éšæœºè·å–ä¸€äº›è¯æ¡è¿›è¡Œè´¨é‡æ£€æŸ¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                SELECT id, lemma, pos, cefr, frequency, notes
                FROM word_lemmas 
                WHERE pos IN ('noun', 'verb')
                ORDER BY RANDOM()
                LIMIT ?
            """, (limit,))
            
            results = cursor.fetchall()
            print(f"ğŸ“‹ éšæœºé€‰æ‹© {len(results)} ä¸ªè¯æ¡è¿›è¡Œè´¨é‡æ£€æŸ¥")
            return results
            
        finally:
            conn.close()
    
    async def process_word_batch(self, words_batch):
        """å¤„ç†ä¸€æ‰¹è¯æ¡"""
        print(f"ğŸ” åˆ†æ {len(words_batch)} ä¸ªè¯æ¡...")
        
        # ä½¿ç”¨OpenAIåˆ†æ
        analyses = await self.analyze_word_validity(words_batch)
        
        if not analyses:
            print("   âŒ æœªèƒ½è·å–åˆ†æç»“æœ")
            return
        
        # å¤„ç†åˆ†æç»“æœ
        for analysis in analyses:
            await self.apply_word_fix(analysis)
            self.stats['words_analyzed'] += 1
    
    async def apply_word_fix(self, analysis):
        """åº”ç”¨è¯æ¡ä¿®å¤"""
        word_id = analysis.get('id')
        lemma = analysis.get('lemma')
        status = analysis.get('status')
        suggestion = analysis.get('suggestion', {})
        action = suggestion.get('action', 'keep')
        reason = analysis.get('reason', '')
        
        print(f"   ğŸ“ {lemma} ({status}): {reason}")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            if action == 'delete':
                # åˆ é™¤è¯æ¡åŠç›¸å…³æ•°æ®
                self.delete_word_completely(cursor, word_id)
                if status == 'english':
                    self.stats['english_words_deleted'] += 1
                else:
                    self.stats['invalid_words_deleted'] += 1
                print(f"     âŒ å·²åˆ é™¤")
                
            elif action == 'redirect':
                # é‡å®šå‘åˆ°ç›®æ ‡è¯æ¡
                target_lemma = suggestion.get('target_lemma')
                if target_lemma:
                    success = self.redirect_to_target(cursor, word_id, lemma, target_lemma)
                    if success:
                        self.stats['plurals_redirected'] += 1
                        print(f"     â†—ï¸  é‡å®šå‘åˆ°: {target_lemma}")
                    else:
                        print(f"     âŒ é‡å®šå‘å¤±è´¥")
                
            elif action == 'correct':
                # ä¿®æ­£è¯æ¡ä¿¡æ¯
                correct_pos = suggestion.get('correct_pos')
                notes = suggestion.get('notes', '')
                if correct_pos:
                    cursor.execute("""
                        UPDATE word_lemmas 
                        SET pos = ?, notes = COALESCE(notes || ' | ', '') || ?
                        WHERE id = ?
                    """, (correct_pos, f"corrected: {notes}", word_id))
                    self.stats['corrections_made'] += 1
                    print(f"     âœï¸  ä¿®æ­£è¯æ€§: {correct_pos}")
            
            conn.commit()
            
        except Exception as e:
            print(f"     âŒ å¤„ç†å¤±è´¥: {e}")
            conn.rollback()
        finally:
            conn.close()
    
    def delete_word_completely(self, cursor, word_id):
        """å®Œå…¨åˆ é™¤è¯æ¡åŠç›¸å…³æ•°æ®"""
        # åˆ é™¤ç›¸å…³æ•°æ®
        cursor.execute("DELETE FROM word_forms WHERE lemma_id = ?", (word_id,))
        cursor.execute("DELETE FROM translations WHERE lemma_id = ?", (word_id,))
        cursor.execute("DELETE FROM examples WHERE lemma_id = ?", (word_id,))
        
        # åˆ é™¤ä¸»è¯æ¡
        cursor.execute("DELETE FROM word_lemmas WHERE id = ?", (word_id,))
    
    def redirect_to_target(self, cursor, word_id, lemma, target_lemma):
        """å°†è¯æ¡é‡å®šå‘åˆ°ç›®æ ‡è¯æ¡"""
        # æŸ¥æ‰¾ç›®æ ‡è¯æ¡
        cursor.execute("SELECT id FROM word_lemmas WHERE lemma = ? AND pos = 'noun'", (target_lemma,))
        target = cursor.fetchone()
        
        if target:
            target_id = target[0]
            
            # è¿ç§»æ•°æ®åˆ°ç›®æ ‡è¯æ¡
            cursor.execute("""
                INSERT OR IGNORE INTO translations (lemma_id, lang_code, text, source)
                SELECT ?, lang_code, text, source 
                FROM translations WHERE lemma_id = ?
            """, (target_id, word_id))
            
            cursor.execute("""
                INSERT OR IGNORE INTO examples (lemma_id, de_text, en_text, zh_text, level)
                SELECT ?, de_text, en_text, zh_text, level
                FROM examples WHERE lemma_id = ?
            """, (target_id, word_id))
            
            # å°†å½“å‰è¯æ¡è½¬ä¸ºè¯å½¢
            cursor.execute("""
                INSERT OR IGNORE INTO word_forms (lemma_id, form, feature_key, feature_value)
                VALUES (?, ?, ?, ?)
            """, (target_id, lemma, "plural", "plural_form"))
            
            # æ›´æ–°ç›®æ ‡è¯æ¡çš„å¤æ•°ä¿¡æ¯
            cursor.execute("""
                UPDATE word_lemmas 
                SET notes = CASE 
                    WHEN notes IS NULL THEN ? 
                    WHEN notes NOT LIKE '%plural:%' THEN notes || ' ' || ?
                    ELSE notes
                END
                WHERE id = ?
            """, (f"plural:{lemma}", f"plural:{lemma}", target_id))
            
            # åˆ é™¤åŸè¯æ¡
            self.delete_word_completely(cursor, word_id)
            return True
        else:
            # ç›®æ ‡è¯æ¡ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»º
            cursor.execute("""
                INSERT INTO word_lemmas (lemma, pos, cefr, notes)
                VALUES (?, 'noun', 'A1', ?)
            """, (target_lemma, f"plural:{lemma}"))
            
            new_target_id = cursor.lastrowid
            
            # è¿ç§»æ•°æ®
            cursor.execute("""
                UPDATE translations SET lemma_id = ? WHERE lemma_id = ?
            """, (new_target_id, word_id))
            
            cursor.execute("""
                UPDATE examples SET lemma_id = ? WHERE lemma_id = ?
            """, (new_target_id, word_id))
            
            cursor.execute("""
                UPDATE word_forms SET lemma_id = ? WHERE lemma_id = ?
            """, (new_target_id, word_id))
            
            # åˆ é™¤åŸè¯æ¡
            cursor.execute("DELETE FROM word_lemmas WHERE id = ?", (word_id,))
            
            return True
    
    async def run_cleaning(self, include_random_check=True):
        """è¿è¡Œæ¸…ç†æµç¨‹"""
        print("ğŸš€ è¯æ¡æ¸…ç†å·¥å…·")
        print("=" * 60)
        
        # 1. åˆ†æå¯ç–‘è¯æ¡
        suspicious_words = self.get_suspicious_words(200)
        
        if suspicious_words:
            print(f"ğŸ” åˆ†æ {len(suspicious_words)} ä¸ªå¯ç–‘è¯æ¡...")
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(suspicious_words), self.batch_size):
                batch = suspicious_words[i:i + self.batch_size]
                await self.process_word_batch(batch)
                
                # å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i + self.batch_size < len(suspicious_words):
                    await asyncio.sleep(self.delay)
        
        # 2. éšæœºè´¨é‡æ£€æŸ¥
        if include_random_check:
            print(f"\nğŸ² éšæœºè´¨é‡æ£€æŸ¥...")
            random_words = self.get_random_words_for_check(30)
            
            if random_words:
                await self.process_word_batch(random_words)
        
        self.print_final_stats()
    
    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        elapsed = datetime.now() - self.stats['start_time']
        
        print(f"\nğŸ‰ è¯æ¡æ¸…ç†å®Œæˆ!")
        print("=" * 50)
        print(f"åˆ†æè¯æ¡æ€»æ•°: {self.stats['words_analyzed']}")
        print(f"åˆ é™¤è‹±è¯­è¯æ¡: {self.stats['english_words_deleted']}")
        print(f"é‡å®šå‘å¤æ•°è¯æ¡: {self.stats['plurals_redirected']}")
        print(f"åˆ é™¤æ— æ•ˆè¯æ¡: {self.stats['invalid_words_deleted']}")
        print(f"ä¿®æ­£è¯æ¡ä¿¡æ¯: {self.stats['corrections_made']}")
        print(f"æ€»ç”¨æ—¶: {elapsed}")
        
        total_changes = (self.stats['english_words_deleted'] + 
                        self.stats['plurals_redirected'] + 
                        self.stats['invalid_words_deleted'] + 
                        self.stats['corrections_made'])
        
        print(f"æ€»æ”¹è¿›é¡¹ç›®: {total_changes}")
        
        if total_changes > 0:
            print(f"\nâœ… æ¸…ç†æˆåŠŸ! æ•°æ®åº“ç°åœ¨æ›´åŠ å¹²å‡€:")
            print("   â€¢ è‹±è¯­è¯æ¡å·²åˆ é™¤")
            print("   â€¢ å¤æ•°è¯æ¡å·²é‡å®šå‘åˆ°å•æ•°") 
            print("   â€¢ æ— æ•ˆè¯æ¡å·²æ¸…ç†")
            print("   â€¢ è¯æ€§é”™è¯¯å·²ä¿®æ­£")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¹ è‡ªåŠ¨è¯æ¡æ¸…ç†å·¥å…·")
    print("=" * 60)
    print("åŠŸèƒ½:")
    print("1. æ£€æµ‹å¹¶åˆ é™¤è‹±è¯­è¯æ¡")
    print("2. å°†å¤æ•°è¯æ¡é‡å®šå‘åˆ°å•æ•°")
    print("3. åˆ é™¤æ— æ•ˆ/é”™è¯¯è¯æ¡")
    print("4. ä¿®æ­£è¯æ€§é”™è¯¯")
    print()
    
    # æ£€æŸ¥OpenAIé…ç½®
    if not settings.openai_api_key:
        print("âŒ é”™è¯¯: æœªé…ç½®OpenAI APIå¯†é’¥")
        return
    
    print("âœ… OpenAIé…ç½®æ­£å¸¸")
    
    cleaner = WordCleaner(batch_size=30, delay=2.0)
    
    try:
        await cleaner.run_cleaning(include_random_check=True)
        
        print(f"\nğŸ’¡ å»ºè®®:")
        print("1. æµ‹è¯•å‰ç«¯æœç´¢åŠŸèƒ½")
        print("2. éªŒè¯å¤æ•°è¯æ¡é‡å®šå‘")
        print("3. æ£€æŸ¥åˆ é™¤çš„è¯æ¡æ˜¯å¦åˆç†")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸…ç†è¢«ç”¨æˆ·ä¸­æ–­")
        cleaner.print_final_stats()
    except Exception as e:
        print(f"\nâŒ æ¸…ç†å¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(main())