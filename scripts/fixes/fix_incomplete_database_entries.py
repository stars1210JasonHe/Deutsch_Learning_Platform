#!/usr/bin/env python3
"""
ä¿®å¤æ•°æ®åº“ä¸­ä¸å®Œæ•´çš„è¯æ±‡æ¡ç›®
ä¸»è¦é—®é¢˜ï¼š
1. 1781ä¸ªè¯æ±‡ç¼ºå°‘ä¸­æ–‡ç¿»è¯‘ (63.0%)
2. 1013ä¸ªè¯æ±‡ç¼ºå°‘ä¾‹å¥ (35.8%)

ä½¿ç”¨OpenAI APIæ‰¹é‡è¡¥å……ç¼ºå¤±ä¿¡æ¯
"""
import sqlite3
import asyncio
import json
import sys
import os
from datetime import datetime
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.getcwd())

try:
    from app.services.openai_service import OpenAIService
    from app.core.config import settings
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥åº”ç”¨æ¨¡å—: {e}")
    sys.exit(1)

class DatabaseFixerService:
    """æ•°æ®åº“ä¿®å¤æœåŠ¡"""
    
    def __init__(self, batch_size=20, delay=2.0):
        self.db_path = 'data/app.db'
        self.openai_service = OpenAIService()
        self.batch_size = batch_size
        self.delay = delay
        self.stats = {
            'words_processed': 0,
            'translations_added': 0,
            'examples_added': 0,
            'errors': 0,
            'skipped': 0,
            'start_time': datetime.now()
        }
        
        print(f"ğŸ”§ æ•°æ®åº“ä¿®å¤æœåŠ¡åˆå§‹åŒ–")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   å»¶è¿Ÿ: {delay}ç§’")
        
    def get_incomplete_words(self, fix_type="missing_chinese", limit=50):
        """è·å–éœ€è¦ä¿®å¤çš„è¯æ±‡åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            if fix_type == "missing_chinese":
                # ç¼ºå°‘ä¸­æ–‡ç¿»è¯‘çš„è¯æ±‡
                cursor.execute("""
                    SELECT wl.id, wl.lemma, wl.pos, wl.cefr
                    FROM word_lemmas wl
                    WHERE wl.id NOT IN (
                        SELECT DISTINCT lemma_id FROM translations 
                        WHERE lang_code = 'zh'
                    )
                    AND wl.id IN (
                        SELECT DISTINCT lemma_id FROM translations 
                        WHERE lang_code = 'en'
                    )
                    ORDER BY 
                        CASE WHEN wl.cefr IN ('A1', 'A2') THEN 1
                             WHEN wl.cefr IN ('B1', 'B2') THEN 2  
                             ELSE 3 END,
                        wl.frequency DESC NULLS LAST,
                        wl.lemma
                    LIMIT ?
                """, (limit,))
                
            elif fix_type == "missing_examples":
                # ç¼ºå°‘ä¾‹å¥çš„è¯æ±‡ï¼ˆæœ‰ç¿»è¯‘ä½†æ— ä¾‹å¥ï¼‰
                cursor.execute("""
                    SELECT wl.id, wl.lemma, wl.pos, wl.cefr
                    FROM word_lemmas wl
                    WHERE wl.id IN (
                        SELECT DISTINCT lemma_id FROM translations
                    )
                    AND wl.id NOT IN (
                        SELECT DISTINCT lemma_id FROM examples 
                        WHERE de_text IS NOT NULL AND de_text != ''
                    )
                    ORDER BY 
                        CASE WHEN wl.cefr IN ('A1', 'A2') THEN 1
                             WHEN wl.cefr IN ('B1', 'B2') THEN 2  
                             ELSE 3 END,
                        wl.frequency DESC NULLS LAST,
                        wl.lemma
                    LIMIT ?
                """, (limit,))
                
            elif fix_type == "high_priority":
                # é«˜ä¼˜å…ˆçº§ï¼šA1/A2çº§åˆ«ä¸”ç¼ºå°‘ä¿¡æ¯çš„è¯æ±‡
                cursor.execute("""
                    SELECT wl.id, wl.lemma, wl.pos, wl.cefr
                    FROM word_lemmas wl
                    WHERE wl.cefr IN ('A1', 'A2')
                    AND (
                        wl.id NOT IN (SELECT DISTINCT lemma_id FROM translations WHERE lang_code = 'zh')
                        OR wl.id NOT IN (SELECT DISTINCT lemma_id FROM examples WHERE de_text IS NOT NULL AND de_text != '')
                    )
                    ORDER BY wl.lemma
                    LIMIT ?
                """, (limit,))
            
            results = cursor.fetchall()
            print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªéœ€è¦ä¿®å¤çš„è¯æ±‡ (ç±»å‹: {fix_type})")
            return results
            
        finally:
            conn.close()
    
    async def generate_chinese_translation(self, lemma, pos, existing_en_translation):
        """ä¸ºè¯æ±‡ç”Ÿæˆä¸­æ–‡ç¿»è¯‘"""
        try:
            prompt = f"""
è¯·ä¸ºå¾·è¯­å•è¯ "{lemma}" ({pos}) æä¾›å‡†ç¡®çš„ä¸­æ–‡ç¿»è¯‘ã€‚

å·²æœ‰è‹±æ–‡ç¿»è¯‘: {existing_en_translation}

è¦æ±‚:
1. æä¾›2-3ä¸ªæœ€å¸¸ç”¨çš„ä¸­æ–‡ç¿»è¯‘
2. ç¿»è¯‘åº”è¯¥å‡†ç¡®ã€ç®€æ´
3. è€ƒè™‘è¯æ€§å’Œè¯­å¢ƒ
4. é¿å…è¿‡äºå¤æ‚æˆ–ç”Ÿåƒ»çš„è¯‘è¯

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›:
{{
    "translations_zh": ["ä¸­æ–‡ç¿»è¯‘1", "ä¸­æ–‡ç¿»è¯‘2", "ä¸­æ–‡ç¿»è¯‘3"]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
            
            response = await self.openai_service._make_request(
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150,
                temperature=0.5
            )
            
            if response and response.get('choices'):
                content = response['choices'][0]['message']['content'].strip()
                data = json.loads(content)
                
                if data.get('translations_zh'):
                    return data['translations_zh']
            
            return None
            
        except Exception as e:
            print(f"   âŒ ç”Ÿæˆä¸­æ–‡ç¿»è¯‘å¤±è´¥: {e}")
            return None
    
    async def generate_example_sentence(self, lemma, pos):
        """ä¸ºè¯æ±‡ç”Ÿæˆä¾‹å¥"""
        try:
            prompt = f"""
ä¸ºå¾·è¯­å•è¯ "{lemma}" ({pos}) ç”Ÿæˆä¸€ä¸ªå®ç”¨çš„ä¾‹å¥ã€‚

è¦æ±‚:
1. ä¾‹å¥åº”è¯¥æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­å¸¸ç”¨çš„åœºæ™¯
2. è¯­æ³•æ­£ç¡®ï¼Œç”¨è¯é€‚å½“  
3. æä¾›å¾·è¯­ã€è‹±è¯­ã€ä¸­æ–‡ä¸‰ä¸ªç‰ˆæœ¬
4. å¾·è¯­ä¾‹å¥é•¿åº¦é€‚ä¸­(6-15ä¸ªå•è¯)
5. ç¡®ä¿ä¾‹å¥ä¸­åŒ…å«ç›®æ ‡å•è¯

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›:
{{
    "de": "å¾·è¯­ä¾‹å¥",
    "en": "English example sentence", 
    "zh": "ä¸­æ–‡ä¾‹å¥"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
            
            response = await self.openai_service._make_request(
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.7
            )
            
            if response and response.get('choices'):
                content = response['choices'][0]['message']['content'].strip()
                data = json.loads(content)
                
                if all(key in data and data[key] for key in ['de', 'en', 'zh']):
                    return data
            
            return None
            
        except Exception as e:
            print(f"   âŒ ç”Ÿæˆä¾‹å¥å¤±è´¥: {e}")
            return None
    
    def add_chinese_translations(self, lemma_id, translations):
        """æ·»åŠ ä¸­æ–‡ç¿»è¯‘åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            for translation in translations:
                cursor.execute("""
                    INSERT INTO translations (lemma_id, lang_code, text, source)
                    VALUES (?, ?, ?, ?)
                """, (lemma_id, "zh", translation.strip(), "openai_batch_fix"))
                
            conn.commit()
            self.stats['translations_added'] += len(translations)
            return True
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜ä¸­æ–‡ç¿»è¯‘å¤±è´¥: {e}")
            conn.rollback()
            return False
            
        finally:
            conn.close()
    
    def add_example_sentence(self, lemma_id, example_data):
        """æ·»åŠ ä¾‹å¥åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                INSERT INTO examples (lemma_id, de_text, en_text, zh_text)
                VALUES (?, ?, ?, ?)
            """, (lemma_id, example_data['de'], example_data['en'], example_data['zh']))
            
            conn.commit()
            self.stats['examples_added'] += 1
            return True
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜ä¾‹å¥å¤±è´¥: {e}")
            conn.rollback()
            return False
            
        finally:
            conn.close()
    
    def get_existing_english_translation(self, lemma_id):
        """è·å–ç°æœ‰çš„è‹±æ–‡ç¿»è¯‘"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                SELECT text FROM translations 
                WHERE lemma_id = ? AND lang_code = 'en'
                LIMIT 1
            """, (lemma_id,))
            
            result = cursor.fetchone()
            return result[0] if result else None
            
        finally:
            conn.close()
    
    async def fix_missing_chinese_translations(self, limit=50):
        """ä¿®å¤ç¼ºå°‘ä¸­æ–‡ç¿»è¯‘çš„è¯æ±‡"""
        print("ğŸ”„ ä¿®å¤ç¼ºå°‘ä¸­æ–‡ç¿»è¯‘çš„è¯æ±‡")
        print("=" * 50)
        
        words_to_fix = self.get_incomplete_words("missing_chinese", limit)
        
        if not words_to_fix:
            print("âœ… æ²¡æœ‰éœ€è¦ä¿®å¤ä¸­æ–‡ç¿»è¯‘çš„è¯æ±‡")
            return
            
        print(f"ğŸ“ å°†å¤„ç† {len(words_to_fix)} ä¸ªè¯æ±‡...")
        
        for i, (lemma_id, lemma, pos, cefr) in enumerate(words_to_fix, 1):
            print(f"[{i}/{len(words_to_fix)}] å¤„ç†: {lemma} ({pos})")
            self.stats['words_processed'] += 1
            
            try:
                # è·å–ç°æœ‰è‹±æ–‡ç¿»è¯‘
                en_translation = self.get_existing_english_translation(lemma_id)
                if not en_translation:
                    print(f"   âš ï¸  è·³è¿‡ï¼šç¼ºå°‘è‹±æ–‡ç¿»è¯‘")
                    self.stats['skipped'] += 1
                    continue
                
                # ç”Ÿæˆä¸­æ–‡ç¿»è¯‘
                zh_translations = await self.generate_chinese_translation(lemma, pos, en_translation)
                
                if zh_translations:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    if self.add_chinese_translations(lemma_id, zh_translations):
                        print(f"   âœ… æ·»åŠ ä¸­æ–‡ç¿»è¯‘: {zh_translations}")
                    else:
                        self.stats['errors'] += 1
                else:
                    print(f"   âŒ æœªèƒ½ç”Ÿæˆä¸­æ–‡ç¿»è¯‘")
                    self.stats['errors'] += 1
                
                # å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i < len(words_to_fix):
                    await asyncio.sleep(self.delay)
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                self.stats['errors'] += 1
                
            # æ¯10ä¸ªè¯æ±‡æ˜¾ç¤ºè¿›åº¦
            if i % 10 == 0:
                print(f"   ğŸ“Š è¿›åº¦: {i}/{len(words_to_fix)} ({i/len(words_to_fix)*100:.1f}%)")
    
    async def fix_missing_examples(self, limit=30):
        """ä¿®å¤ç¼ºå°‘ä¾‹å¥çš„è¯æ±‡"""
        print("ğŸ”„ ä¿®å¤ç¼ºå°‘ä¾‹å¥çš„è¯æ±‡")
        print("=" * 50)
        
        words_to_fix = self.get_incomplete_words("missing_examples", limit)
        
        if not words_to_fix:
            print("âœ… æ²¡æœ‰éœ€è¦ä¿®å¤ä¾‹å¥çš„è¯æ±‡")
            return
            
        print(f"ğŸ“ å°†å¤„ç† {len(words_to_fix)} ä¸ªè¯æ±‡...")
        
        for i, (lemma_id, lemma, pos, cefr) in enumerate(words_to_fix, 1):
            print(f"[{i}/{len(words_to_fix)}] å¤„ç†: {lemma} ({pos})")
            self.stats['words_processed'] += 1
            
            try:
                # ç”Ÿæˆä¾‹å¥
                example_data = await self.generate_example_sentence(lemma, pos)
                
                if example_data:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    if self.add_example_sentence(lemma_id, example_data):
                        print(f"   âœ… æ·»åŠ ä¾‹å¥: {example_data['de']}")
                    else:
                        self.stats['errors'] += 1
                else:
                    print(f"   âŒ æœªèƒ½ç”Ÿæˆä¾‹å¥")
                    self.stats['errors'] += 1
                
                # å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i < len(words_to_fix):
                    await asyncio.sleep(self.delay)
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                self.stats['errors'] += 1
                
            # æ¯5ä¸ªè¯æ±‡æ˜¾ç¤ºè¿›åº¦
            if i % 5 == 0:
                print(f"   ğŸ“Š è¿›åº¦: {i}/{len(words_to_fix)} ({i/len(words_to_fix)*100:.1f}%)")
    
    async def fix_high_priority_words(self, limit=25):
        """ä¿®å¤é«˜ä¼˜å…ˆçº§è¯æ±‡ï¼ˆA1/A2çº§åˆ«ï¼‰"""
        print("ğŸ”„ ä¿®å¤é«˜ä¼˜å…ˆçº§è¯æ±‡ (A1/A2)")
        print("=" * 50)
        
        words_to_fix = self.get_incomplete_words("high_priority", limit)
        
        if not words_to_fix:
            print("âœ… æ²¡æœ‰éœ€è¦ä¿®å¤çš„é«˜ä¼˜å…ˆçº§è¯æ±‡")
            return
            
        print(f"ğŸ“ å°†å¤„ç† {len(words_to_fix)} ä¸ªA1/A2çº§åˆ«è¯æ±‡...")
        
        for i, (lemma_id, lemma, pos, cefr) in enumerate(words_to_fix, 1):
            print(f"[{i}/{len(words_to_fix)}] å¤„ç†: {lemma} ({pos}) [{cefr}]")
            self.stats['words_processed'] += 1
            
            try:
                # æ£€æŸ¥éœ€è¦ä»€ä¹ˆç±»å‹çš„ä¿®å¤
                needs_chinese = self.needs_chinese_translation(lemma_id)
                needs_example = self.needs_example_sentence(lemma_id)
                
                # ä¿®å¤ä¸­æ–‡ç¿»è¯‘
                if needs_chinese:
                    en_translation = self.get_existing_english_translation(lemma_id)
                    if en_translation:
                        zh_translations = await self.generate_chinese_translation(lemma, pos, en_translation)
                        if zh_translations and self.add_chinese_translations(lemma_id, zh_translations):
                            print(f"   âœ… æ·»åŠ ä¸­æ–‡ç¿»è¯‘")
                
                # ä¿®å¤ä¾‹å¥
                if needs_example:
                    example_data = await self.generate_example_sentence(lemma, pos)
                    if example_data and self.add_example_sentence(lemma_id, example_data):
                        print(f"   âœ… æ·»åŠ ä¾‹å¥")
                
                # å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i < len(words_to_fix):
                    await asyncio.sleep(self.delay)
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                self.stats['errors'] += 1
    
    def needs_chinese_translation(self, lemma_id):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–‡ç¿»è¯‘"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                SELECT COUNT(*) FROM translations 
                WHERE lemma_id = ? AND lang_code = 'zh'
            """, (lemma_id,))
            
            return cursor.fetchone()[0] == 0
            
        finally:
            conn.close()
    
    def needs_example_sentence(self, lemma_id):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ä¾‹å¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                SELECT COUNT(*) FROM examples 
                WHERE lemma_id = ? AND de_text IS NOT NULL AND de_text != ''
            """, (lemma_id,))
            
            return cursor.fetchone()[0] == 0
            
        finally:
            conn.close()
    
    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        elapsed = datetime.now() - self.stats['start_time']
        
        print(f"\nğŸ‰ æ•°æ®åº“ä¿®å¤å®Œæˆ!")
        print("=" * 50)
        print(f"å¤„ç†è¯æ±‡: {self.stats['words_processed']}")
        print(f"æ·»åŠ ç¿»è¯‘: {self.stats['translations_added']}")
        print(f"æ·»åŠ ä¾‹å¥: {self.stats['examples_added']}")
        print(f"è·³è¿‡: {self.stats['skipped']}")
        print(f"é”™è¯¯: {self.stats['errors']}")
        print(f"æ€»ç”¨æ—¶: {elapsed}")
        
        success_rate = ((self.stats['translations_added'] + self.stats['examples_added']) / 
                       max(self.stats['words_processed'], 1) * 100)
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        print(f"\nğŸ’¡ å»ºè®®:")
        print("1. é‡æ–°è¿è¡Œå®Œæ•´æ€§æ£€æŸ¥ç¡®è®¤ä¿®å¤æ•ˆæœ")
        print("2. æµ‹è¯•å‰ç«¯æœç´¢åŠŸèƒ½éªŒè¯æ–°å¢å†…å®¹")
        print("3. å¦‚éœ€ç»§ç»­ä¿®å¤ï¼Œå¯å¢åŠ æ‰¹æ¬¡å¤§å°")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä¿®å¤æ•°æ®åº“ä¸å®Œæ•´è¯æ±‡æ¡ç›®')
    parser.add_argument('--mode', 
                       choices=['chinese', 'examples', 'priority', 'all'], 
                       default='priority',
                       help='ä¿®å¤æ¨¡å¼ (é»˜è®¤: priority)')
    parser.add_argument('--batch-size', type=int, default=30, 
                       help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 30)')
    parser.add_argument('--delay', type=float, default=2.0,
                       help='APIè°ƒç”¨å»¶è¿Ÿ(ç§’) (é»˜è®¤: 2.0)')
    parser.add_argument('--limit', type=int, default=50,
                       help='å¤„ç†è¯æ±‡æ•°é‡é™åˆ¶ (é»˜è®¤: 50)')
    parser.add_argument('--dry-run', action='store_true',
                       help='è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ•°æ®åº“')
    
    args = parser.parse_args()
    
    print("ğŸ”§ æ•°æ®åº“ä¸å®Œæ•´æ¡ç›®ä¿®å¤å·¥å…·")
    print("=" * 60)
    print(f"âš™ï¸  é…ç½®:")
    print(f"   ä¿®å¤æ¨¡å¼: {args.mode}")
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"   APIå»¶è¿Ÿ: {args.delay}ç§’")
    print(f"   å¤„ç†é™åˆ¶: {args.limit}ä¸ªè¯æ±‡")
    print(f"   è¯•è¿è¡Œ: {'æ˜¯' if args.dry_run else 'å¦'}")
    
    if args.dry_run:
        print("\nâš ï¸  è¯•è¿è¡Œæ¨¡å¼ï¼šä¸ä¼šå®é™…ä¿®æ”¹æ•°æ®åº“")
        # å¯ä»¥æ·»åŠ è¯•è¿è¡Œé€»è¾‘
        return
    
    # æ£€æŸ¥OpenAIé…ç½®
    if not settings.openai_api_key:
        print("\nâŒ é”™è¯¯: æœªé…ç½®OpenAI APIå¯†é’¥")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
        return
    
    print(f"\nâœ… OpenAIé…ç½®æ£€æŸ¥é€šè¿‡")
    print(f"   æ¨¡å‹: {settings.openai_model}")
    print(f"   APIåœ°å€: {settings.openai_base_url}")
    
    fixer = DatabaseFixerService(
        batch_size=args.batch_size,
        delay=args.delay
    )
    
    print()
    
    try:
        if args.mode == 'chinese':
            await fixer.fix_missing_chinese_translations(args.limit)
        elif args.mode == 'examples':
            await fixer.fix_missing_examples(args.limit)
        elif args.mode == 'priority':
            await fixer.fix_high_priority_words(args.limit)
        elif args.mode == 'all':
            print("ğŸ“‹ æ‰§è¡Œå®Œæ•´ä¿®å¤æµç¨‹...")
            await fixer.fix_high_priority_words(20)
            await fixer.fix_missing_chinese_translations(30)
            await fixer.fix_missing_examples(20)
        
        fixer.print_final_stats()
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ä¿®å¤è¢«ç”¨æˆ·ä¸­æ–­")
        fixer.print_final_stats()
    except Exception as e:
        print(f"\nâŒ ä¿®å¤è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        print(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(main())