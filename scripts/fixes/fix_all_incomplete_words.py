#!/usr/bin/env python3
"""
ä¿®å¤æ•°æ®åº“ä¸­æ‰€æœ‰ä¸å®Œæ•´çš„è¯æ±‡
ä½¿ç”¨OpenAIæ‰¹é‡è¡¥å…¨æ‰€æœ‰ç¼ºå¤±æ•°æ®ï¼Œé¿å…æœªæ¥é‡å¤è°ƒç”¨
"""
import asyncio
import sys
import os
import sqlite3
import json
import re
from dotenv import load_dotenv
from datetime import datetime

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(".env")
sys.path.append(os.getcwd())

from app.services.lexicon_llm_service import LexiconLLMService

class DatabaseWordFixer:
    """æ•°æ®åº“è¯æ±‡ä¿®å¤å™¨"""
    
    def __init__(self):
        self.llm_service = LexiconLLMService()
        self.db_path = 'data/app.db'
        self.stats = {
            'total_processed': 0,
            'nouns_fixed': 0,
            'verbs_fixed': 0,
            'adjectives_fixed': 0,
            'other_fixed': 0,
            'already_complete': 0,
            'failed': 0,
            'start_time': datetime.now()
        }
    
    def sanitize_for_json(self, data):
        """æ¸…ç†æ•°æ®ä¸­çš„æ— æ•ˆæ§åˆ¶å­—ç¬¦ï¼Œä½¿å…¶å¯ä»¥è¢«JSONåºåˆ—åŒ–"""
        if isinstance(data, str):
            # ç§»é™¤æˆ–æ›¿æ¢æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ã€å›è½¦ç¬¦ï¼‰
            return re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', data)
        elif isinstance(data, dict):
            return {k: self.sanitize_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self.sanitize_for_json(item) for item in data]
        else:
            return data
    
    def safe_json_dumps(self, data):
        """å®‰å…¨çš„JSONåºåˆ—åŒ–ï¼Œå¸¦æœ‰é”™è¯¯å¤„ç†"""
        try:
            sanitized_data = self.sanitize_for_json(data)
            return json.dumps(sanitized_data, ensure_ascii=False)
        except (TypeError, ValueError) as e:
            print(f"   âš ï¸ JSONåºåˆ—åŒ–å¤±è´¥: {e}")
            # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªç©ºçš„JSONå¯¹è±¡
            return "{}"
    
    def get_incomplete_words(self, limit=None):
        """è·å–æ‰€æœ‰ä¸å®Œæ•´çš„è¯æ±‡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰ä¸å®Œæ•´çš„è¯æ±‡
            query = """
                SELECT 
                    wl.id as lemma_id,
                    wl.lemma,
                    wl.pos as legacy_pos,
                    ls.id as sense_id,
                    ls.upos,
                    ls.gender,
                    ls.gloss_en,
                    ls.gloss_zh,
                    ls.confidence,
                    np.gen_sg,
                    np.plural,
                    vp.aux,
                    vp.partizip_ii,
                    COUNT(fu.id) as form_count,
                    COUNT(ex.id) as example_count
                FROM word_lemmas wl
                JOIN lemma_senses ls ON ls.lemma_id = wl.id
                LEFT JOIN noun_props np ON np.sense_id = ls.id
                LEFT JOIN verb_props vp ON vp.sense_id = ls.id
                LEFT JOIN forms_unimorph fu ON fu.sense_id = ls.id
                LEFT JOIN examples ex ON ex.sense_id = ls.id AND ex.de_text IS NOT NULL AND ex.de_text != ''
                WHERE 
                    -- åŸºç¡€é—®é¢˜ï¼šè¯æ€§é”™è¯¯æˆ–ç¼ºå°‘é‡Šä¹‰
                    (ls.upos IN ('UNKNOWN', 'OTHER') OR ls.upos IS NULL)
                    OR (ls.gloss_en IS NULL OR ls.gloss_zh IS NULL)
                    OR (ls.confidence < 0.8)
                    -- åè¯é—®é¢˜ï¼šç¼ºå°‘æ€§åˆ«æˆ–å¤æ•°
                    OR (ls.upos = 'NOUN' AND (ls.gender IS NULL OR np.plural IS NULL))
                    -- åŠ¨è¯é—®é¢˜ï¼šç¼ºå°‘åŠ©åŠ¨è¯æˆ–è¿‡å»åˆ†è¯
                    OR (ls.upos = 'VERB' AND (vp.aux IS NULL OR vp.partizip_ii IS NULL))
                    -- ç¼ºå°‘ä¾‹å¥
                    OR (ex.id IS NULL)
                GROUP BY wl.id, ls.id
                ORDER BY 
                    -- ä¼˜å…ˆå¤„ç†æ˜æ˜¾é”™è¯¯çš„è¯æ±‡
                    CASE 
                        WHEN ls.upos IN ('UNKNOWN', 'OTHER') THEN 1
                        WHEN ls.gloss_en IS NULL OR ls.gloss_zh IS NULL THEN 2
                        WHEN ls.upos = 'VERB' AND vp.aux IS NULL THEN 3
                        WHEN ls.upos = 'NOUN' AND ls.gender IS NULL THEN 4
                        ELSE 5
                    END,
                    wl.lemma
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            cursor.execute(query)
            columns = [desc[0] for desc in cursor.description]
            
            results = []
            for row in cursor.fetchall():
                results.append(dict(zip(columns, row)))
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªéœ€è¦ä¿®å¤çš„è¯æ¡")
            return results
            
        finally:
            conn.close()
    
    async def fix_word(self, word_data):
        """ä¿®å¤å•ä¸ªè¯æ±‡"""
        lemma = word_data['lemma']
        upos = word_data['upos']
        sense_id = word_data['sense_id']
        
        try:
            print(f"\nğŸ”§ ä¿®å¤: {lemma} ({upos})")
            
            # åˆ¤æ–­éœ€è¦ä¿®å¤çš„ç±»å‹
            needs_basic_fix = (
                upos in ('UNKNOWN', 'OTHER') or 
                not word_data.get('gloss_en') or 
                not word_data.get('gloss_zh') or
                word_data.get('confidence', 0) < 0.8
            )
            
            if needs_basic_fix:
                # éœ€è¦é‡æ–°è¯†åˆ«è¯æ€§å’ŒåŸºç¡€ä¿¡æ¯
                await self._fix_basic_word_info(word_data)
                # é‡æ–°è·å–æ›´æ–°åçš„æ•°æ®
                word_data = await self._get_updated_word_data(word_data['lemma_id'])
                upos = word_data['upos']
            
            # æ ¹æ®è¯æ€§è¿›è¡Œä¸“é—¨ä¿®å¤
            if upos == 'NOUN':
                await self._fix_noun(word_data)
                self.stats['nouns_fixed'] += 1
            elif upos == 'VERB':
                await self._fix_verb(word_data)
                self.stats['verbs_fixed'] += 1
            elif upos in ('ADJ', 'ADJECTIVE'):
                await self._fix_adjective(word_data)
                self.stats['adjectives_fixed'] += 1
            else:
                await self._fix_other(word_data)
                self.stats['other_fixed'] += 1
            
            print(f"   âœ… ä¿®å¤å®Œæˆ")
            
        except Exception as e:
            print(f"   âŒ ä¿®å¤å¤±è´¥: {e}")
            self.stats['failed'] += 1
        finally:
            self.stats['total_processed'] += 1
    
    async def _fix_basic_word_info(self, word_data):
        """ä¿®å¤åŸºç¡€è¯æ±‡ä¿¡æ¯ï¼ˆè¯æ€§è¯†åˆ«ï¼‰"""
        lemma = word_data['lemma']
        
        # è°ƒç”¨OpenAIé‡æ–°åˆ†æè¯æ±‡
        print(f"   ğŸ¤– é‡æ–°åˆ†æè¯æ±‡åŸºç¡€ä¿¡æ¯...")
        
        # ä½¿ç”¨è¯æ±‡æ¶ˆæ­§åŠŸèƒ½
        disambiguation = await self.llm_service.disambiguate_lemma(lemma)
        
        if disambiguation and disambiguation.get('senses'):
            # å–ç¬¬ä¸€ä¸ªï¼ˆæœ€å¯èƒ½çš„ï¼‰è¯æ€§
            primary_sense = disambiguation['senses'][0]
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            try:
                # æ›´æ–°è¯æ€§
                new_upos = primary_sense.get('upos', 'OTHER')
                new_xpos = primary_sense.get('xpos', 'OTHER')
                
                cursor.execute("""
                    UPDATE word_lemmas 
                    SET pos = ?
                    WHERE id = ?
                """, (new_upos.lower(), word_data['lemma_id']))
                
                cursor.execute("""
                    UPDATE lemma_senses 
                    SET upos = ?, xpos = ?, gender = ?, gloss_en = ?, gloss_zh = ?, 
                        confidence = 0.85, source = 'openai_batch_fix'
                    WHERE id = ?
                """, (
                    new_upos, new_xpos,
                    primary_sense.get('gender'),
                    primary_sense.get('gloss_en'),
                    primary_sense.get('gloss_zh'),
                    word_data['sense_id']
                ))
                
                conn.commit()
                print(f"   âœ… æ›´æ–°è¯æ€§: {new_upos}")
                
            finally:
                conn.close()
    
    async def _get_updated_word_data(self, lemma_id):
        """è·å–æ›´æ–°åçš„è¯æ±‡æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                SELECT 
                    wl.id as lemma_id, wl.lemma, wl.pos as legacy_pos,
                    ls.id as sense_id, ls.upos, ls.gender, ls.gloss_en, ls.gloss_zh, ls.confidence
                FROM word_lemmas wl
                JOIN lemma_senses ls ON ls.lemma_id = wl.id
                WHERE wl.id = ?
            """, (lemma_id,))
            
            row = cursor.fetchone()
            if row:
                columns = [desc[0] for desc in cursor.description]
                return dict(zip(columns, row))
            return None
            
        finally:
            conn.close()
    
    async def _fix_noun(self, word_data):
        """ä¿®å¤åè¯"""
        if word_data.get('gender') and word_data.get('plural'):
            print(f"   âš ï¸ åè¯å·²å®Œæ•´ï¼Œè·³è¿‡")
            self.stats['already_complete'] += 1
            return
        
        lemma = word_data['lemma']
        context = {'existing_data': word_data}
        
        print(f"   ğŸ¤– è·å–åè¯å±æ€§...")
        noun_data = await self.llm_service.enrich_noun(lemma, context)
        
        if noun_data:
            await self._save_noun_data(word_data['sense_id'], noun_data)
    
    async def _fix_verb(self, word_data):
        """ä¿®å¤åŠ¨è¯"""
        if word_data.get('aux') and word_data.get('partizip_ii'):
            print(f"   âš ï¸ åŠ¨è¯å·²å®Œæ•´ï¼Œè·³è¿‡")
            self.stats['already_complete'] += 1
            return
        
        lemma = word_data['lemma']
        context = {'existing_data': word_data}
        
        print(f"   ğŸ¤– è·å–åŠ¨è¯å±æ€§...")
        verb_data = await self.llm_service.enrich_verb(lemma, context)
        
        if verb_data:
            await self._save_verb_data(word_data['sense_id'], verb_data)
    
    async def _fix_adjective(self, word_data):
        """ä¿®å¤å½¢å®¹è¯"""
        lemma = word_data['lemma']
        context = {'existing_data': word_data}
        
        print(f"   ğŸ¤– è·å–å½¢å®¹è¯å±æ€§...")
        adj_data = await self.llm_service.enrich_adjective(lemma, context)
        
        if adj_data:
            await self._save_adjective_data(word_data['sense_id'], adj_data)
    
    async def _fix_other(self, word_data):
        """ä¿®å¤å…¶ä»–è¯æ€§"""
        # å¯¹äºå…¶ä»–è¯æ€§ï¼Œåªç¡®ä¿æœ‰åŸºç¡€çš„é‡Šä¹‰å’Œä¾‹å¥
        if word_data.get('gloss_en') and word_data.get('gloss_zh') and word_data.get('example_count', 0) > 0:
            print(f"   âš ï¸ å…¶ä»–è¯æ€§å·²å®Œæ•´ï¼Œè·³è¿‡")
            self.stats['already_complete'] += 1
            return
        
        # ç”ŸæˆåŸºç¡€ä¾‹å¥
        await self._ensure_example(word_data)
    
    async def _save_noun_data(self, sense_id, data):
        """ä¿å­˜åè¯æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æ›´æ–°senseä¿¡æ¯
            cursor.execute("""
                UPDATE lemma_senses 
                SET gender = COALESCE(gender, ?),
                    gloss_en = COALESCE(gloss_en, ?),
                    gloss_zh = COALESCE(gloss_zh, ?),
                    confidence = MAX(confidence, 0.9)
                WHERE id = ?
            """, (
                data.get('gender'),
                data.get('gloss_en'),
                data.get('gloss_zh'),
                sense_id
            ))
            
            # ä¿å­˜åè¯å±æ€§
            noun_props = data.get('noun_props', {})
            if noun_props:
                cursor.execute("""
                    INSERT OR REPLACE INTO noun_props 
                    (sense_id, gen_sg, plural, declension_class, dative_plural_ends_n)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    sense_id,
                    noun_props.get('gen_sg'),
                    noun_props.get('plural'),
                    noun_props.get('declension_class'),
                    noun_props.get('dative_plural_ends_n', False)
                ))
            
            # ä¿å­˜è¯å½¢å’Œä¾‹å¥
            await self._save_forms_and_examples(cursor, sense_id, data)
            conn.commit()
            
        finally:
            conn.close()
    
    async def _save_verb_data(self, sense_id, data):
        """ä¿å­˜åŠ¨è¯æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æ›´æ–°senseä¿¡æ¯
            cursor.execute("""
                UPDATE lemma_senses 
                SET gloss_en = COALESCE(gloss_en, ?),
                    gloss_zh = COALESCE(gloss_zh, ?),
                    confidence = MAX(confidence, 0.9)
                WHERE id = ?
            """, (
                data.get('gloss_en'),
                data.get('gloss_zh'),
                sense_id
            ))
            
            # ä¿å­˜åŠ¨è¯å±æ€§
            cursor.execute("""
                INSERT OR REPLACE INTO verb_props 
                (sense_id, separable, prefix, aux, regularity, partizip_ii, reflexive, valency_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                sense_id,
                data.get('separable', False),
                data.get('prefix'),
                data.get('aux'),
                data.get('regularity'),
                data.get('partizip_ii'),
                data.get('reflexive', False),
                self.safe_json_dumps(data.get('valency', {}))
            ))
            
            # ä¿å­˜å˜ä½è¡¨
            await self._save_verb_conjugations(cursor, sense_id, data.get('tables', {}))
            
            # ä¿å­˜ä¾‹å¥
            await self._save_forms_and_examples(cursor, sense_id, data)
            conn.commit()
            
        finally:
            conn.close()
    
    async def _save_adjective_data(self, sense_id, data):
        """ä¿å­˜å½¢å®¹è¯æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æ›´æ–°senseä¿¡æ¯
            cursor.execute("""
                UPDATE lemma_senses 
                SET gloss_en = COALESCE(gloss_en, ?),
                    gloss_zh = COALESCE(gloss_zh, ?),
                    confidence = MAX(confidence, 0.9)
                WHERE id = ?
            """, (
                data.get('gloss_en'),
                data.get('gloss_zh'),
                sense_id
            ))
            
            # ä¿å­˜ä¾‹å¥
            await self._save_forms_and_examples(cursor, sense_id, data)
            conn.commit()
            
        finally:
            conn.close()
    
    async def _save_verb_conjugations(self, cursor, sense_id, tables):
        """ä¿å­˜åŠ¨è¯å˜ä½"""
        for tense, forms in tables.items():
            if isinstance(forms, dict):
                for person, form in forms.items():
                    if form:
                        cursor.execute("""
                            INSERT OR IGNORE INTO forms_unimorph
                            (sense_id, form, features_json)
                            VALUES (?, ?, ?)
                        """, (
                            sense_id,
                            form,
                            self.safe_json_dumps({
                                "POS": "VERB",
                                "Tense": tense.title(),
                                "Person": person
                            })
                        ))
    
    async def _save_forms_and_examples(self, cursor, sense_id, data):
        """ä¿å­˜è¯å½¢å˜åŒ–å’Œä¾‹å¥"""
        # ä¿å­˜è¯å½¢
        forms = data.get('forms', [])
        for form_data in forms:
            cursor.execute("""
                INSERT OR IGNORE INTO forms_unimorph
                (sense_id, form, features_json)
                VALUES (?, ?, ?)
            """, (
                sense_id,
                form_data.get('form'),
                self.safe_json_dumps(form_data.get('features', {}))
            ))
        
        # ä¿å­˜ä¾‹å¥
        example = data.get('example')
        if example and example.get('de'):
            # å…ˆæ£€æŸ¥æ˜¯å¦å·²æœ‰ä¾‹å¥
            cursor.execute("""
                SELECT COUNT(*) FROM examples 
                WHERE sense_id = ? AND de_text IS NOT NULL AND de_text != ''
            """, (sense_id,))
            
            if cursor.fetchone()[0] == 0:
                cursor.execute("""
                    INSERT INTO examples
                    (lemma_id, sense_id, de_text, en_text, zh_text, level)
                    SELECT ls.lemma_id, ?, ?, ?, ?, 'A1'
                    FROM lemma_senses ls 
                    WHERE ls.id = ?
                """, (
                    sense_id,
                    example.get('de'),
                    example.get('en'),
                    example.get('zh'),
                    sense_id
                ))
    
    async def _ensure_example(self, word_data):
        """ç¡®ä¿æœ‰ä¾‹å¥"""
        if word_data['example_count'] > 0:
            return
        
        # ç®€å•ç”Ÿæˆä¾‹å¥çš„é€»è¾‘å¯ä»¥åœ¨è¿™é‡Œå®ç°
        pass
    
    def print_progress(self, current, total):
        """æ‰“å°è¿›åº¦"""
        percentage = (current / total * 100) if total > 0 else 0
        elapsed = datetime.now() - self.stats['start_time']
        
        print(f"\nğŸ“Š è¿›åº¦: {current}/{total} ({percentage:.1f}%)")
        print(f"   å·²ä¿®å¤åè¯: {self.stats['nouns_fixed']}")
        print(f"   å·²ä¿®å¤åŠ¨è¯: {self.stats['verbs_fixed']}")
        print(f"   å·²ä¿®å¤å½¢å®¹è¯: {self.stats['adjectives_fixed']}")
        print(f"   å·²ä¿®å¤å…¶ä»–: {self.stats['other_fixed']}")
        print(f"   å·²å®Œæ•´: {self.stats['already_complete']}")
        print(f"   å¤±è´¥: {self.stats['failed']}")
        print(f"   ç”¨æ—¶: {elapsed}")
    
    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        elapsed = datetime.now() - self.stats['start_time']
        
        print(f"\nğŸ‰ æ‰¹é‡ä¿®å¤å®Œæˆï¼")
        print(f"=" * 50)
        print(f"æ€»å¤„ç†è¯æ¡: {self.stats['total_processed']}")
        print(f"ä¿®å¤åè¯: {self.stats['nouns_fixed']}")
        print(f"ä¿®å¤åŠ¨è¯: {self.stats['verbs_fixed']}")
        print(f"ä¿®å¤å½¢å®¹è¯: {self.stats['adjectives_fixed']}")
        print(f"ä¿®å¤å…¶ä»–: {self.stats['other_fixed']}")
        print(f"å·²å®Œæ•´: {self.stats['already_complete']}")
        print(f"å¤±è´¥: {self.stats['failed']}")
        print(f"æ€»ç”¨æ—¶: {elapsed}")

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡ä¿®å¤æ•°æ®åº“ä¸­çš„ä¸å®Œæ•´è¯æ±‡')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†çš„è¯æ¡æ•°é‡')
    parser.add_argument('--batch-size', type=int, default=10, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--dry-run', action='store_true', help='è¯•è¿è¡Œæ¨¡å¼')
    
    args = parser.parse_args()
    
    print("ğŸš€ æ‰¹é‡ä¿®å¤æ•°æ®åº“è¯æ±‡")
    print("=" * 50)
    
    if args.dry_run:
        print("âš ï¸ è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šä¿å­˜æ•°æ®")
    
    fixer = DatabaseWordFixer()
    
    # è·å–éœ€è¦ä¿®å¤çš„è¯æ±‡
    incomplete_words = fixer.get_incomplete_words(args.limit)
    
    if not incomplete_words:
        print("âœ… æ‰€æœ‰è¯æ±‡éƒ½å·²å®Œæ•´ï¼")
        return
    
    print(f"å¼€å§‹æ‰¹é‡ä¿®å¤ {len(incomplete_words)} ä¸ªè¯æ¡...")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    
    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…APIé™åˆ¶
    for i in range(0, len(incomplete_words), args.batch_size):
        batch = incomplete_words[i:i + args.batch_size]
        
        print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//args.batch_size + 1} ({len(batch)} ä¸ªè¯æ¡)")
        
        for j, word_data in enumerate(batch):
            if not args.dry_run:
                await fixer.fix_word(word_data)
            else:
                print(f"   è¯•è¿è¡Œ: {word_data['lemma']} ({word_data['upos']})")
            
            # æ¯ä¸ªè¯ä¹‹é—´ç¨ä½œå»¶è¿Ÿ
            await asyncio.sleep(0.5)
        
        # æ‰¹æ¬¡é—´ä¼‘æ¯
        if i + args.batch_size < len(incomplete_words):
            print(f"ğŸ’¤ æ‰¹æ¬¡å®Œæˆï¼Œä¼‘æ¯1ç§’...")
            await asyncio.sleep(1)
        
        # æ˜¾ç¤ºè¿›åº¦
        fixer.print_progress(min(i + args.batch_size, len(incomplete_words)), len(incomplete_words))
    
    fixer.print_final_stats()

if __name__ == "__main__":
    asyncio.run(main())
