#!/usr/bin/env python3
"""
Âæ∑ËØ≠ËØçÂ∫ìÊâπÈáèÂõûÂ°´ËÑöÊú¨
‰ΩøÁî®OpenAIË°•ÂÖ®Áé∞ÊúâËØçÊù°ÁöÑÁº∫Â§±Â≠óÊÆµ
"""
import asyncio
import sys
import os
import sqlite3
import json
import argparse
from typing import List, Dict, Any, Optional
from datetime import datetime
from dotenv import load_dotenv

# Âä†ËΩΩÁéØÂ¢ÉÂèòÈáè
load_dotenv(" - Copy.env")
sys.path.append(os.getcwd())

from app.services.lexicon_llm_service import LexiconLLMService

class LexiconBackfillService:
    """ËØçÂ∫ìÂõûÂ°´ÊúçÂä°"""
    
    def __init__(self):
        self.llm_service = LexiconLLMService()
        self.db_path = 'data/app.db'
        
        # ÁªüËÆ°‰ø°ÊÅØ
        self.stats = {
            'processed': 0,
            'enriched_nouns': 0,
            'enriched_verbs': 0,
            'enriched_adjectives': 0,
            'failed': 0,
            'skipped': 0,
            'start_time': datetime.now()
        }
    
    def get_incomplete_lemmas(self, limit: int = None, offset: int = 0) -> List[Dict[str, Any]]:
        """Ëé∑ÂèñÈúÄË¶ÅË°•ÂÖ®ÁöÑËØçÊù°"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # Êü•ÊâæÁº∫Â∞ëÈáçË¶ÅÂ≠óÊÆµÁöÑËØçÊù°
            query = """
                SELECT 
                    wl.id as lemma_id,
                    wl.lemma,
                    wl.pos,
                    ls.id as sense_id,
                    ls.upos,
                    ls.gender,
                    ls.gloss_en,
                    ls.gloss_zh,
                    np.gen_sg,
                    np.plural,
                    vp.aux,
                    vp.partizip_ii,
                    COUNT(fu.id) as form_count
                FROM word_lemmas wl
                JOIN lemma_senses ls ON ls.lemma_id = wl.id
                LEFT JOIN noun_props np ON np.sense_id = ls.id
                LEFT JOIN verb_props vp ON vp.sense_id = ls.id
                LEFT JOIN forms_unimorph fu ON fu.sense_id = ls.id
                WHERE 
                    -- ÂêçËØçÁº∫Â∞ëÊÄßÂà´ÊàñÂ§çÊï∞
                    (ls.upos = 'NOUN' AND (ls.gender IS NULL OR np.plural IS NULL))
                    OR
                    -- Âä®ËØçÁº∫Â∞ëÂä©Âä®ËØçÊàñËøáÂéªÂàÜËØç
                    (ls.upos = 'VERB' AND (vp.aux IS NULL OR vp.partizip_ii IS NULL))
                    OR
                    -- Áº∫Â∞ëËã±ÊñáÊàñ‰∏≠ÊñáÈáä‰πâ
                    (ls.gloss_en IS NULL OR ls.gloss_zh IS NULL)
                    OR
                    -- Áº∫Â∞ëËØçÂΩ¢ÂèòÂåñ
                    (fu.id IS NULL)
                GROUP BY wl.id, ls.id
                ORDER BY 
                    CASE wl.pos 
                        WHEN 'noun' THEN 1
                        WHEN 'verb' THEN 2
                        WHEN 'adjective' THEN 3
                        ELSE 4
                    END,
                    wl.lemma
            """
            
            if limit:
                query += f" LIMIT {limit} OFFSET {offset}"
            
            cursor.execute(query)
            columns = [desc[0] for desc in cursor.description]
            
            results = []
            for row in cursor.fetchall():
                results.append(dict(zip(columns, row)))
            
            print(f"üìä ÊâæÂà∞ {len(results)} ‰∏™ÈúÄË¶ÅË°•ÂÖ®ÁöÑËØçÊù°")
            return results
            
        finally:
            conn.close()
    
    async def enrich_lemma(self, lemma_data: Dict[str, Any]) -> bool:
        """Ë°•ÂÖ®Âçï‰∏™ËØçÊù°"""
        lemma = lemma_data['lemma']
        upos = lemma_data['upos']
        sense_id = lemma_data['sense_id']
        
        try:
            print(f"\nüîç Â§ÑÁêÜ: {lemma} ({upos})")
            
            if upos == 'NOUN':
                return await self._enrich_noun(lemma_data)
            elif upos == 'VERB':
                return await self._enrich_verb(lemma_data)
            elif upos == 'ADJ':
                return await self._enrich_adjective(lemma_data)
            else:
                print(f"   ‚ö†Ô∏è Ë∑≥Ëøá‰∏çÊîØÊåÅÁöÑËØçÊÄß: {upos}")
                self.stats['skipped'] += 1
                return False
                
        except Exception as e:
            print(f"   ‚ùå Â§ÑÁêÜÂ§±Ë¥•: {e}")
            self.stats['failed'] += 1
            return False
        finally:
            self.stats['processed'] += 1
    
    async def _enrich_noun(self, lemma_data: Dict[str, Any]) -> bool:
        """Ë°•ÂÖ®ÂêçËØç"""
        lemma = lemma_data['lemma']
        sense_id = lemma_data['sense_id']
        
        # Ê£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅË°•ÂÖ®
        needs_enrichment = (
            not lemma_data['gender'] or 
            not lemma_data['plural'] or
            not lemma_data['gloss_en'] or
            not lemma_data['gloss_zh'] or
            lemma_data['form_count'] == 0
        )
        
        if not needs_enrichment:
            print(f"   ‚úÖ {lemma} Â∑≤ÂÆåÊï¥ÔºåË∑≥Ëøá")
            self.stats['skipped'] += 1
            return True
        
        # ÊûÑÂª∫‰∏ä‰∏ãÊñá
        context = {}
        if lemma_data.get('gender'):
            context['existing_gender'] = lemma_data['gender']
        
        # Ë∞ÉÁî®LLMËé∑ÂèñË°•ÂÖ®Êï∞ÊçÆ
        print(f"   ü§ñ Ë∞ÉÁî®LLMË°•ÂÖ®ÂêçËØçÊï∞ÊçÆ...")
        enrichment_data = await self.llm_service.enrich_noun(lemma, context)
        
        if not enrichment_data:
            print(f"   ‚ùå LLMËøîÂõûÁ©∫Êï∞ÊçÆ")
            return False
        
        # ‰øùÂ≠òÂà∞Êï∞ÊçÆÂ∫ì
        return await self._save_noun_data(sense_id, enrichment_data)
    
    async def _enrich_verb(self, lemma_data: Dict[str, Any]) -> bool:
        """Ë°•ÂÖ®Âä®ËØç"""
        lemma = lemma_data['lemma']
        sense_id = lemma_data['sense_id']
        
        # Ê£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅË°•ÂÖ®
        needs_enrichment = (
            not lemma_data['aux'] or
            not lemma_data['partizip_ii'] or
            not lemma_data['gloss_en'] or
            not lemma_data['gloss_zh'] or
            lemma_data['form_count'] == 0
        )
        
        if not needs_enrichment:
            print(f"   ‚úÖ {lemma} Â∑≤ÂÆåÊï¥ÔºåË∑≥Ëøá")
            self.stats['skipped'] += 1
            return True
        
        # Ë∞ÉÁî®LLMËé∑ÂèñË°•ÂÖ®Êï∞ÊçÆ
        print(f"   ü§ñ Ë∞ÉÁî®LLMË°•ÂÖ®Âä®ËØçÊï∞ÊçÆ...")
        enrichment_data = await self.llm_service.enrich_verb(lemma)
        
        if not enrichment_data:
            print(f"   ‚ùå LLMËøîÂõûÁ©∫Êï∞ÊçÆ")
            return False
        
        # ‰øùÂ≠òÂà∞Êï∞ÊçÆÂ∫ì
        return await self._save_verb_data(sense_id, enrichment_data)
    
    async def _enrich_adjective(self, lemma_data: Dict[str, Any]) -> bool:
        """Ë°•ÂÖ®ÂΩ¢ÂÆπËØç"""
        lemma = lemma_data['lemma']
        sense_id = lemma_data['sense_id']
        
        # Ë∞ÉÁî®LLMËé∑ÂèñË°•ÂÖ®Êï∞ÊçÆ
        print(f"   ü§ñ Ë∞ÉÁî®LLMË°•ÂÖ®ÂΩ¢ÂÆπËØçÊï∞ÊçÆ...")
        enrichment_data = await self.llm_service.enrich_adjective(lemma)
        
        if not enrichment_data:
            print(f"   ‚ùå LLMËøîÂõûÁ©∫Êï∞ÊçÆ")
            return False
        
        # ‰øùÂ≠òÂü∫Á°ÄÊï∞ÊçÆ
        return await self._save_adjective_data(sense_id, enrichment_data)
    
    async def _save_noun_data(self, sense_id: int, data: Dict[str, Any]) -> bool:
        """‰øùÂ≠òÂêçËØçÊï∞ÊçÆÂà∞Êï∞ÊçÆÂ∫ì"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # Êõ¥Êñ∞ lemma_senses
            cursor.execute("""
                UPDATE lemma_senses 
                SET gender = COALESCE(gender, ?),
                    gloss_en = COALESCE(gloss_en, ?),
                    gloss_zh = COALESCE(gloss_zh, ?),
                    confidence = 0.8,
                    source = 'openai_backfill'
                WHERE id = ?
            """, (
                data.get('gender'),
                data.get('gloss_en'),
                data.get('gloss_zh'),
                sense_id
            ))
            
            # ÊèíÂÖ•ÊàñÊõ¥Êñ∞ noun_props
            noun_props = data.get('noun_props', {})
            if noun_props:
                cursor.execute("""
                    INSERT OR REPLACE INTO noun_props 
                    (sense_id, gen_sg, plural, declension_class, dative_plural_ends_n)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    sense_id,
                    noun_props.get('gen_sg'),
                    noun_props.get('plural'),
                    noun_props.get('declension_class'),
                    noun_props.get('dative_plural_ends_n', False)
                ))
            
            # ÊèíÂÖ•ËØçÂΩ¢ÂèòÂåñ
            forms = data.get('forms', [])
            for form_data in forms:
                cursor.execute("""
                    INSERT OR IGNORE INTO forms_unimorph
                    (sense_id, form, features_json)
                    VALUES (?, ?, ?)
                """, (
                    sense_id,
                    form_data.get('form'),
                    json.dumps(form_data.get('features', {}))
                ))
            
            # ÊèíÂÖ•‰æãÂè•ÔºàÂ¶ÇÊûú‰∏çÂ≠òÂú®Ôºâ
            example = data.get('example')
            if example:
                cursor.execute("""
                    INSERT OR IGNORE INTO examples
                    (lemma_id, sense_id, de_text, en_text, zh_text, level)
                    SELECT ls.lemma_id, ?, ?, ?, ?, 'A1'
                    FROM lemma_senses ls 
                    WHERE ls.id = ?
                    AND NOT EXISTS (
                        SELECT 1 FROM examples e2 
                        WHERE e2.sense_id = ? AND e2.de_text = ?
                    )
                """, (
                    sense_id,
                    example.get('de'),
                    example.get('en'),
                    example.get('zh'),
                    sense_id,
                    sense_id,
                    example.get('de')
                ))
            
            conn.commit()
            self.stats['enriched_nouns'] += 1
            print(f"   ‚úÖ ÂêçËØçÊï∞ÊçÆÂ∑≤‰øùÂ≠ò")
            return True
            
        except Exception as e:
            print(f"   ‚ùå ‰øùÂ≠òÂêçËØçÊï∞ÊçÆÂ§±Ë¥•: {e}")
            conn.rollback()
            return False
        finally:
            conn.close()
    
    async def _save_verb_data(self, sense_id: int, data: Dict[str, Any]) -> bool:
        """‰øùÂ≠òÂä®ËØçÊï∞ÊçÆÂà∞Êï∞ÊçÆÂ∫ì"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # Êõ¥Êñ∞ lemma_senses
            cursor.execute("""
                UPDATE lemma_senses 
                SET gloss_en = COALESCE(gloss_en, ?),
                    gloss_zh = COALESCE(gloss_zh, ?),
                    confidence = 0.8,
                    source = 'openai_backfill'
                WHERE id = ?
            """, (
                data.get('gloss_en'),
                data.get('gloss_zh'),
                sense_id
            ))
            
            # ÊèíÂÖ•ÊàñÊõ¥Êñ∞ verb_props
            cursor.execute("""
                INSERT OR REPLACE INTO verb_props 
                (sense_id, separable, prefix, aux, regularity, partizip_ii, reflexive, valency_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                sense_id,
                data.get('separable', False),
                data.get('prefix'),
                data.get('aux'),
                data.get('regularity'),
                data.get('partizip_ii'),
                data.get('reflexive', False),
                json.dumps(data.get('valency', {}))
            ))
            
            # ÊèíÂÖ•Âä®ËØçÂèò‰ΩçÂΩ¢Âºè
            tables = data.get('tables', {})
            
            # Pr√§sens
            praesens = tables.get('praesens', {})
            for person, form in praesens.items():
                if form:
                    cursor.execute("""
                        INSERT OR IGNORE INTO forms_unimorph
                        (sense_id, form, features_json)
                        VALUES (?, ?, ?)
                    """, (
                        sense_id,
                        form,
                        json.dumps({"POS": "VERB", "Tense": "Pres", "Person": person})
                    ))
            
            # Pr√§teritum
            praeteritum = tables.get('praeteritum', {})
            for person, form in praeteritum.items():
                if form:
                    cursor.execute("""
                        INSERT OR IGNORE INTO forms_unimorph
                        (sense_id, form, features_json)
                        VALUES (?, ?, ?)
                    """, (
                        sense_id,
                        form,
                        json.dumps({"POS": "VERB", "Tense": "Past", "Person": person})
                    ))
            
            # ÊèíÂÖ•‰æãÂè•
            example = data.get('example')
            if example:
                cursor.execute("""
                    INSERT OR IGNORE INTO examples
                    (lemma_id, sense_id, de_text, en_text, zh_text, level)
                    SELECT ls.lemma_id, ?, ?, ?, ?, 'A1'
                    FROM lemma_senses ls 
                    WHERE ls.id = ?
                    AND NOT EXISTS (
                        SELECT 1 FROM examples e2 
                        WHERE e2.sense_id = ? AND e2.de_text = ?
                    )
                """, (
                    sense_id,
                    example.get('de'),
                    example.get('en'),
                    example.get('zh'),
                    sense_id,
                    sense_id,
                    example.get('de')
                ))
            
            conn.commit()
            self.stats['enriched_verbs'] += 1
            print(f"   ‚úÖ Âä®ËØçÊï∞ÊçÆÂ∑≤‰øùÂ≠ò")
            return True
            
        except Exception as e:
            print(f"   ‚ùå ‰øùÂ≠òÂä®ËØçÊï∞ÊçÆÂ§±Ë¥•: {e}")
            conn.rollback()
            return False
        finally:
            conn.close()
    
    async def _save_adjective_data(self, sense_id: int, data: Dict[str, Any]) -> bool:
        """‰øùÂ≠òÂΩ¢ÂÆπËØçÊï∞ÊçÆÂà∞Êï∞ÊçÆÂ∫ì"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # Êõ¥Êñ∞Âü∫Á°Ä‰ø°ÊÅØ
            cursor.execute("""
                UPDATE lemma_senses 
                SET gloss_en = COALESCE(gloss_en, ?),
                    gloss_zh = COALESCE(gloss_zh, ?),
                    confidence = 0.8,
                    source = 'openai_backfill'
                WHERE id = ?
            """, (
                data.get('gloss_en'),
                data.get('gloss_zh'),
                sense_id
            ))
            
            # ÊèíÂÖ•‰æãÂè•
            example = data.get('example')
            if example:
                cursor.execute("""
                    INSERT OR IGNORE INTO examples
                    (lemma_id, sense_id, de_text, en_text, zh_text, level)
                    SELECT ls.lemma_id, ?, ?, ?, ?, 'A1'
                    FROM lemma_senses ls 
                    WHERE ls.id = ?
                """, (
                    sense_id,
                    example.get('de'),
                    example.get('en'),
                    example.get('zh'),
                    sense_id
                ))
            
            conn.commit()
            self.stats['enriched_adjectives'] += 1
            print(f"   ‚úÖ ÂΩ¢ÂÆπËØçÊï∞ÊçÆÂ∑≤‰øùÂ≠ò")
            return True
            
        except Exception as e:
            print(f"   ‚ùå ‰øùÂ≠òÂΩ¢ÂÆπËØçÊï∞ÊçÆÂ§±Ë¥•: {e}")
            conn.rollback()
            return False
        finally:
            conn.close()
    
    def print_stats(self):
        """ÊâìÂç∞ÁªüËÆ°‰ø°ÊÅØ"""
        elapsed = datetime.now() - self.stats['start_time']
        
        print(f"\nüìä ÂõûÂ°´ÁªüËÆ°:")
        print(f"   Â§ÑÁêÜËØçÊù°: {self.stats['processed']}")
        print(f"   Ë°•ÂÖ®ÂêçËØç: {self.stats['enriched_nouns']}")
        print(f"   Ë°•ÂÖ®Âä®ËØç: {self.stats['enriched_verbs']}")
        print(f"   Ë°•ÂÖ®ÂΩ¢ÂÆπËØç: {self.stats['enriched_adjectives']}")
        print(f"   Ë∑≥Ëøá: {self.stats['skipped']}")
        print(f"   Â§±Ë¥•: {self.stats['failed']}")
        print(f"   Áî®Êó∂: {elapsed}")

async def main():
    """‰∏ªÂáΩÊï∞"""
    parser = argparse.ArgumentParser(description='Âæ∑ËØ≠ËØçÂ∫ìÊâπÈáèÂõûÂ°´')
    parser.add_argument('--limit', type=int, default=50, help='Â§ÑÁêÜËØçÊù°Êï∞ÈáèÈôêÂà∂')
    parser.add_argument('--offset', type=int, default=0, help='ÂºÄÂßãÂÅèÁßªÈáè')
    parser.add_argument('--dry-run', action='store_true', help='ËØïËøêË°åÔºå‰∏ç‰øùÂ≠òÊï∞ÊçÆ')
    
    args = parser.parse_args()
    
    print("üöÄ Âæ∑ËØ≠ËØçÂ∫ìÊâπÈáèÂõûÂ°´")
    print("=" * 50)
    
    if args.dry_run:
        print("‚ö†Ô∏è ËØïËøêË°åÊ®°Âºè - ‰∏ç‰ºö‰øùÂ≠òÊï∞ÊçÆ")
    
    service = LexiconBackfillService()
    
    # Ëé∑ÂèñÈúÄË¶ÅË°•ÂÖ®ÁöÑËØçÊù°
    incomplete_lemmas = service.get_incomplete_lemmas(args.limit, args.offset)
    
    if not incomplete_lemmas:
        print("‚úÖ ÊâÄÊúâËØçÊù°ÈÉΩÂ∑≤ÂÆåÊï¥ÔºåÊó†ÈúÄÂõûÂ°´")
        return
    
    print(f"ÂºÄÂßãÂ§ÑÁêÜ {len(incomplete_lemmas)} ‰∏™ËØçÊù°...")
    
    # ÊâπÈáèÂ§ÑÁêÜ
    for i, lemma_data in enumerate(incomplete_lemmas, 1):
        print(f"\n[{i}/{len(incomplete_lemmas)}]", end=" ")
        
        if not args.dry_run:
            await service.enrich_lemma(lemma_data)
        else:
            print(f"ËØïËøêË°å: {lemma_data['lemma']} ({lemma_data['upos']})")
        
        # ÊØè10‰∏™ËØçÊù°‰ºëÊÅØ‰∏Ä‰∏ãÔºåÈÅøÂÖçAPIÈôêÂà∂
        if i % 10 == 0:
            print(f"\nüí§ Â∑≤Â§ÑÁêÜ {i} ‰∏™ËØçÊù°Ôºå‰ºëÊÅØ3Áßí...")
            await asyncio.sleep(3)
    
    service.print_stats()
    print("\nüéâ ÊâπÈáèÂõûÂ°´ÂÆåÊàêÔºÅ")

if __name__ == "__main__":
    asyncio.run(main())
