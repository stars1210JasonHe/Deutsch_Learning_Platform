"""
æ”¹è¿›çš„Excelè¯æ±‡å¯¼å…¥å™¨
åŸºäºå®é™…Excelæ–‡ä»¶ç»“æ„ï¼Œæ”¯æŒè‡ªåŠ¨è¡¥å…¨ç¼ºå¤±çš„è¯­æ³•å½¢å¼
"""
import asyncio
import sys
import os
import re
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from app.db.session import SessionLocal
from app.models.word import WordLemma, Translation, Example, WordForm
from app.services.openai_service import OpenAIService
import zipfile
import xml.etree.ElementTree as ET


class ImprovedExcelImporter:
    def __init__(self):
        self.db = SessionLocal()
        self.openai_service = OpenAIService()
        self.statistics = {
            'imported': 0,
            'skipped': 0,
            'enhanced': 0,
            'errors': 0
        }

    def parse_xlsx_simple(self, file_path):
        """ç®€å•è§£æXLSXæ–‡ä»¶"""
        
        words_data = []
        
        try:
            with zipfile.ZipFile(file_path, 'r') as zip_ref:
                # è¯»å–å…±äº«å­—ç¬¦ä¸²
                shared_strings = []
                if 'xl/sharedStrings.xml' in zip_ref.namelist():
                    with zip_ref.open('xl/sharedStrings.xml') as f:
                        content = f.read().decode('utf-8')
                        root = ET.fromstring(content)
                        for si in root.findall('.//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t'):
                            shared_strings.append(si.text if si.text else "")
                
                # è¯»å–å·¥ä½œè¡¨æ•°æ®
                if 'xl/worksheets/sheet1.xml' in zip_ref.namelist():
                    with zip_ref.open('xl/worksheets/sheet1.xml') as f:
                        content = f.read().decode('utf-8')
                        root = ET.fromstring(content)
                        
                        # è§£ææ‰€æœ‰è¡Œ
                        rows = []
                        for row in root.findall('.//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}row'):
                            row_data = []
                            for cell in row.findall('.//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}c'):
                                value = ""
                                v_element = cell.find('.//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}v')
                                if v_element is not None:
                                    if cell.get('t') == 's':
                                        try:
                                            string_index = int(v_element.text)
                                            if string_index < len(shared_strings):
                                                value = shared_strings[string_index]
                                        except (ValueError, IndexError):
                                            value = v_element.text if v_element.text else ""
                                    else:
                                        value = v_element.text if v_element.text else ""
                                row_data.append(value)
                            if row_data:  # åªæ·»åŠ éç©ºè¡Œ
                                rows.append(row_data)
                        
                        return rows
                        
        except Exception as e:
            print(f"âŒ è§£æExcelæ–‡ä»¶å¤±è´¥: {e}")
            return []

    def process_b1_file(self, file_path):
        """å¤„ç†B1çº§åˆ«çš„Excelæ–‡ä»¶ï¼ˆæ ‡å‡†æ ¼å¼ï¼‰"""
        
        print(f"ğŸ“š å¤„ç†B1æ–‡ä»¶: {os.path.basename(file_path)}")
        
        rows = self.parse_xlsx_simple(file_path)
        if not rows:
            return []
        
        # B1æ–‡ä»¶çš„åˆ—ç»“æ„ï¼š
        # 0: German Word, 1: Article, 2: Noun Only, 3: Translation, 4: Example Sentence, 5: Classification, 6: Page Number
        
        words_data = []
        headers = rows[0] if rows else []
        print(f"æ£€æµ‹åˆ°çš„æ ‡é¢˜: {headers}")
        
        for row in rows[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
            if len(row) >= 6:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
                word_info = {
                    'german_word': row[0].strip() if row[0] else '',
                    'article': row[1].strip() if len(row) > 1 and row[1] else '',
                    'noun_only': row[2].strip() if len(row) > 2 and row[2] else '',
                    'translation': row[3].strip() if len(row) > 3 and row[3] else '',
                    'example_de': row[4].strip() if len(row) > 4 and row[4] else '',
                    'classification': row[5].strip() if len(row) > 5 and row[5] else '',
                    'pos': 'noun',  # B1æ–‡ä»¶ä¸»è¦æ˜¯åè¯
                    'level': 'B1'
                }
                
                if word_info['german_word']:  # åªå¤„ç†æœ‰å¾·è¯­è¯æ±‡çš„è¡Œ
                    words_data.append(word_info)
        
        print(f"ä»B1æ–‡ä»¶æå–äº† {len(words_data)} ä¸ªè¯æ±‡")
        return words_data

    def determine_pos_from_classification(self, classification):
        """æ ¹æ®åˆ†ç±»ç¡®å®šè¯æ€§"""
        
        if not classification:
            return 'noun'  # é»˜è®¤ä¸ºåè¯
        
        classification_lower = classification.lower()
        
        # åŠ¨è¯æ ‡è¯†
        if any(keyword in classification_lower for keyword in ['verb', 'action', 'åŠ¨è¯']):
            return 'verb'
        
        # å½¢å®¹è¯æ ‡è¯†
        if any(keyword in classification_lower for keyword in ['adjective', 'adj', 'quality', 'å½¢å®¹è¯']):
            return 'adjective'
        
        # å‰¯è¯æ ‡è¯†
        if any(keyword in classification_lower for keyword in ['adverb', 'adv', 'å‰¯è¯']):
            return 'adverb'
        
        # å…¶ä»–éƒ½å½“ä½œåè¯
        return 'noun'

    async def import_word_to_database(self, word_info):
        """å°†å•è¯å¯¼å…¥æ•°æ®åº“"""
        
        german_word = word_info.get('german_word', '').strip()
        if not german_word:
            return False
        
        try:
            # æ¸…ç†å¾·è¯­å•è¯ï¼Œæå–lemma
            lemma = self._extract_lemma(german_word, word_info.get('article', ''))
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = self.db.query(WordLemma).filter(
                WordLemma.lemma.ilike(lemma)
            ).first()
            
            if existing:
                print(f"â© '{lemma}' å·²å­˜åœ¨ï¼Œè·³è¿‡")
                self.statistics['skipped'] += 1
                return False
            
            # ç¡®å®šè¯æ€§
            pos = self.determine_pos_from_classification(word_info.get('classification', ''))
            
            # åˆ›å»ºè¯æ¡
            word = WordLemma(
                lemma=lemma,
                pos=pos,
                cefr=word_info.get('level', 'B1'),
                notes=f"Imported from Excel - {word_info.get('classification', 'unknown')}"
            )
            
            self.db.add(word)
            self.db.commit()
            self.db.refresh(word)
            
            # æ·»åŠ è‹±æ–‡ç¿»è¯‘
            translation_text = word_info.get('translation', '').strip()
            if translation_text:
                translation = Translation(
                    lemma_id=word.id,
                    lang_code="en",
                    text=translation_text,
                    source="excel_import"
                )
                self.db.add(translation)
            
            # æ·»åŠ ä¾‹å¥
            example_de = word_info.get('example_de', '').strip()
            if example_de:
                example = Example(
                    lemma_id=word.id,
                    de_text=example_de,
                    level=word_info.get('level', 'B1')
                )
                self.db.add(example)
            
            # å¤„ç†åè¯çš„å† è¯ä¿¡æ¯
            article = word_info.get('article', '').strip()
            if pos == 'noun' and article and article.lower() in ['der', 'die', 'das']:
                article_form = WordForm(
                    lemma_id=word.id,
                    form=f"{article} {lemma}",
                    feature_key="article",
                    feature_value=article
                )
                self.db.add(article_form)
                
                # å¦‚æœæœ‰"Noun Only"ä¿¡æ¯ï¼Œä¹Ÿä¿å­˜
                noun_only = word_info.get('noun_only', '').strip()
                if noun_only and noun_only != lemma:
                    expansion_form = WordForm(
                        lemma_id=word.id,
                        form=noun_only,
                        feature_key="expansion",
                        feature_value="full_form"
                    )
                    self.db.add(expansion_form)
            
            self.db.commit()
            
            # ä½¿ç”¨OpenAIè¡¥å…¨ç¼ºå¤±ä¿¡æ¯ï¼ˆç‰¹åˆ«æ˜¯åŠ¨è¯å’Œç¼ºå°‘ä¸­æ–‡ç¿»è¯‘çš„è¯ï¼‰
            enhanced = await self._enhance_with_openai(word, word_info)
            
            print(f"âœ… å¯¼å…¥: {lemma} ({pos})" + (" [å¢å¼º]" if enhanced else ""))
            self.statistics['imported'] += 1
            if enhanced:
                self.statistics['enhanced'] += 1
            
            return True
            
        except Exception as e:
            print(f"âŒ å¯¼å…¥ '{german_word}' å¤±è´¥: {e}")
            self.statistics['errors'] += 1
            self.db.rollback()
            return False

    def _extract_lemma(self, german_word, article):
        """æå–è¯æ±‡çš„lemmaå½¢å¼"""
        
        german_word = german_word.strip()
        
        # å¦‚æœè¯æ±‡ä»¥å† è¯å¼€å¤´ï¼Œå»æ‰å† è¯
        for art in ['der ', 'die ', 'das ', 'Der ', 'Die ', 'Das ']:
            if german_word.startswith(art):
                return german_word[len(art):].strip()
        
        return german_word

    async def _enhance_with_openai(self, word: WordLemma, word_info: dict):
        """ä½¿ç”¨OpenAIå¢å¼ºè¯æ±‡ä¿¡æ¯"""
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢å¼º
            needs_chinese = not any(t.lang_code == 'zh' for t in word.translations)
            needs_verb_forms = word.pos == 'verb'
            
            if not (needs_chinese or needs_verb_forms):
                return False
            
            print(f"ğŸ” OpenAIå¢å¼º: {word.lemma}")
            analysis = await self.openai_service.analyze_word(word.lemma)
            
            enhanced = False
            
            # æ·»åŠ ä¸­æ–‡ç¿»è¯‘
            if needs_chinese:
                translations_zh = analysis.get("translations_zh", [])
                for trans in translations_zh:
                    translation = Translation(
                        lemma_id=word.id,
                        lang_code="zh",
                        text=trans,
                        source="openai_enhancement"
                    )
                    self.db.add(translation)
                    enhanced = True
            
            # æ·»åŠ åŠ¨è¯å˜ä½
            if needs_verb_forms and analysis.get("tables"):
                tables = analysis["tables"]
                for tense, forms in tables.items():
                    if isinstance(forms, dict):
                        for person, form in forms.items():
                            if form and person not in ["aux", "partizip_ii"]:
                                word_form = WordForm(
                                    lemma_id=word.id,
                                    form=form,
                                    feature_key="tense",
                                    feature_value=f"{tense}_{person}"
                                )
                                self.db.add(word_form)
                                enhanced = True
            
            # å¦‚æœæ²¡æœ‰ä¾‹å¥ä¸”OpenAIæä¾›äº†ä¾‹å¥ï¼Œæ·»åŠ è‹±æ–‡å’Œä¸­æ–‡ç¿»è¯‘
            if not word.examples and analysis.get("example"):
                example_data = analysis["example"]
                if word.examples:  # æ›´æ–°ç°æœ‰ä¾‹å¥
                    existing_example = word.examples[0]
                    if not existing_example.en_text:
                        existing_example.en_text = example_data.get("en", "")
                    if not existing_example.zh_text:
                        existing_example.zh_text = example_data.get("zh", "")
                    enhanced = True
            
            if enhanced:
                self.db.commit()
            
            # APIé€Ÿç‡é™åˆ¶
            await asyncio.sleep(1)
            
            return enhanced
            
        except Exception as e:
            print(f"âš ï¸ OpenAIå¢å¼ºå¤±è´¥ '{word.lemma}': {e}")
            return False

    async def import_file(self, file_path, max_words=None):
        """å¯¼å…¥å•ä¸ªæ–‡ä»¶"""
        
        level = "B1"  # ç›®å‰ä¸»è¦å¤„ç†B1æ–‡ä»¶
        if "A1" in file_path:
            level = "A1"
        elif "A2" in file_path:
            level = "A2"
        
        print(f"\nğŸ“š å¯¼å…¥æ–‡ä»¶: {os.path.basename(file_path)} (çº§åˆ«: {level})")
        print("=" * 60)
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†æ–¹æ³•
        if "B1" in file_path:
            words_data = self.process_b1_file(file_path)
        else:
            print("âš ï¸ A1/A2æ–‡ä»¶æ ¼å¼æš‚æœªå®ç°ï¼Œè·³è¿‡")
            return
        
        if not words_data:
            print("âŒ æœªæ‰¾åˆ°å¯å¯¼å…¥çš„è¯æ±‡æ•°æ®")
            return
        
        # é™åˆ¶å¯¼å…¥æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if max_words:
            words_data = words_data[:max_words]
            print(f"é™åˆ¶å¯¼å…¥å‰ {max_words} ä¸ªè¯æ±‡")
        
        print(f"å‡†å¤‡å¯¼å…¥ {len(words_data)} ä¸ªè¯æ±‡...")
        
        # æ‰¹é‡å¯¼å…¥
        for i, word_info in enumerate(words_data):
            print(f"\nå¤„ç† {i+1}/{len(words_data)}: {word_info.get('german_word', 'N/A')}")
            await self.import_word_to_database(word_info)
            
            # æ¯10ä¸ªè¯æ±‡ä¼‘æ¯ä¸€ä¸‹
            if (i + 1) % 10 == 0:
                print(f"å·²å¤„ç† {i+1} ä¸ªè¯æ±‡ï¼Œä¼‘æ¯2ç§’...")
                await asyncio.sleep(2)
        
        print(f"\nâœ… æ–‡ä»¶å¯¼å…¥å®Œæˆ: {os.path.basename(file_path)}")

    async def import_all_files(self, max_words_per_file=None):
        """å¯¼å…¥æ‰€æœ‰æ–‡ä»¶"""
        
        files_to_process = [
            "/mnt/e/LanguageLearning/german_vocabulary_B1_sample.xlsx"  # å…ˆåªå¤„ç†B1æ–‡ä»¶
        ]
        
        print("ğŸš€ å¼€å§‹æ‰¹é‡å¯¼å…¥Excelè¯æ±‡æ–‡ä»¶")
        print("=" * 60)
        
        for file_path in files_to_process:
            if os.path.exists(file_path):
                await self.import_file(file_path, max_words_per_file)
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # æ˜¾ç¤ºç»Ÿè®¡
        print(f"\nğŸ“Š å¯¼å…¥ç»Ÿè®¡:")
        print(f"   - æˆåŠŸå¯¼å…¥: {self.statistics['imported']}")
        print(f"   - è·³è¿‡å·²å­˜åœ¨: {self.statistics['skipped']}")
        print(f"   - OpenAIå¢å¼º: {self.statistics['enhanced']}")
        print(f"   - é”™è¯¯: {self.statistics['errors']}")
        
        self.db.close()


async def main():
    """ä¸»å‡½æ•°"""
    
    importer = ImprovedExcelImporter()
    
    print("Excelè¯æ±‡å¯¼å…¥å™¨")
    print("æœ¬æ¬¡å¯¼å…¥å°†å¤„ç†B1çº§åˆ«çš„Excelæ–‡ä»¶")
    print("æ¯ä¸ªè¯æ±‡éƒ½ä¼šå°è¯•ä½¿ç”¨OpenAIè¡¥å…¨ä¸­æ–‡ç¿»è¯‘å’ŒåŠ¨è¯å˜ä½")
    print()
    
    # è¯¢é—®æ˜¯å¦é™åˆ¶å¯¼å…¥æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    try:
        max_words = input("é™åˆ¶å¯¼å…¥æ•°é‡ï¼ˆå›è½¦è·³è¿‡ï¼Œæ•°å­—é™åˆ¶ï¼‰: ").strip()
        max_words = int(max_words) if max_words else None
    except ValueError:
        max_words = None
    
    if max_words:
        print(f"å°†é™åˆ¶æ¯ä¸ªæ–‡ä»¶å¯¼å…¥å‰ {max_words} ä¸ªè¯æ±‡")
    else:
        print("å°†å¯¼å…¥æ‰€æœ‰è¯æ±‡ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´å’ŒAPIè´¹ç”¨ï¼‰")
    
    print("\nå¼€å§‹å¯¼å…¥...")
    await importer.import_all_files(max_words)
    
    print("\nğŸ‰ å¯¼å…¥å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())