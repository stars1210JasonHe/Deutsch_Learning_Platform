"""
è¯æ±‡ç®¡ç†è„šæœ¬ - ç”¨äºæ‰¹é‡ç®¡ç†è¯åº“
"""
import asyncio
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from app.db.session import SessionLocal
from app.models.word import WordLemma, Translation, Example
from app.services.openai_service import OpenAIService


class VocabularyManager:
    def __init__(self):
        self.db = SessionLocal()
        self.openai_service = OpenAIService()

    async def batch_add_words(self, word_list: list[str]):
        """æ‰¹é‡æ·»åŠ è¯æ±‡åˆ°æ•°æ®åº“"""
        
        print(f"ğŸ“š å¼€å§‹æ‰¹é‡æ·»åŠ  {len(word_list)} ä¸ªè¯æ±‡...")
        
        success_count = 0
        skip_count = 0
        error_count = 0
        
        for word in word_list:
            try:
                word = word.strip()
                if not word:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = self.db.query(WordLemma).filter(
                    WordLemma.lemma.ilike(word)
                ).first()
                
                if existing:
                    print(f"â© '{word}' å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    skip_count += 1
                    continue
                
                # è°ƒç”¨OpenAIåˆ†æ
                print(f"ğŸ” åˆ†æè¯æ±‡: {word}")
                analysis = await self.openai_service.analyze_word(word)
                
                # åˆ›å»ºè¯æ¡
                lemma = WordLemma(
                    lemma=analysis.get("lemma", word),
                    pos=analysis.get("pos"),
                    notes=f"Batch imported via OpenAI analysis"
                )
                
                self.db.add(lemma)
                self.db.commit()
                self.db.refresh(lemma)
                
                # æ·»åŠ ç¿»è¯‘
                translations_en = analysis.get("translations_en", [])
                translations_zh = analysis.get("translations_zh", [])
                
                for trans in translations_en:
                    translation = Translation(
                        lemma_id=lemma.id,
                        lang_code="en",
                        text=trans,
                        source="openai_batch"
                    )
                    self.db.add(translation)
                
                for trans in translations_zh:
                    translation = Translation(
                        lemma_id=lemma.id,
                        lang_code="zh", 
                        text=trans,
                        source="openai_batch"
                    )
                    self.db.add(translation)
                
                # æ·»åŠ ä¾‹å¥
                example_data = analysis.get("example")
                if example_data:
                    example = Example(
                        lemma_id=lemma.id,
                        de_text=example_data.get("de", ""),
                        en_text=example_data.get("en", ""),
                        zh_text=example_data.get("zh", ""),
                        level="A1"
                    )
                    self.db.add(example)
                
                self.db.commit()
                print(f"âœ… æ·»åŠ æˆåŠŸ: {word} ({analysis.get('pos', 'unknown')})")
                success_count += 1
                
                # é¿å…APIé€Ÿç‡é™åˆ¶
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"âŒ å¤„ç† '{word}' æ—¶å‡ºé”™: {e}")
                error_count += 1
                self.db.rollback()
        
        print(f"\nğŸ“Š æ‰¹é‡æ·»åŠ å®Œæˆ:")
        print(f"   - æˆåŠŸæ·»åŠ : {success_count}")
        print(f"   - è·³è¿‡å·²å­˜åœ¨: {skip_count}")
        print(f"   - é”™è¯¯: {error_count}")
        
        self.db.close()

    def show_stats(self):
        """æ˜¾ç¤ºè¯åº“ç»Ÿè®¡ä¿¡æ¯"""
        
        total_words = self.db.query(WordLemma).count()
        total_translations = self.db.query(Translation).count()
        total_examples = self.db.query(Example).count()
        
        verbs = self.db.query(WordLemma).filter(WordLemma.pos == "verb").count()
        nouns = self.db.query(WordLemma).filter(WordLemma.pos == "noun").count()
        adjectives = self.db.query(WordLemma).filter(WordLemma.pos == "adjective").count()
        
        print("ğŸ“Š è¯åº“ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - è¯æ±‡æ€»æ•°: {total_words}")
        print(f"   - ç¿»è¯‘æ€»æ•°: {total_translations}")
        print(f"   - ä¾‹å¥æ€»æ•°: {total_examples}")
        print(f"   - åŠ¨è¯: {verbs}")
        print(f"   - åè¯: {nouns}") 
        print(f"   - å½¢å®¹è¯: {adjectives}")
        
        # æ˜¾ç¤ºæœ€è¿‘æ·»åŠ çš„è¯æ±‡
        recent_words = self.db.query(WordLemma).order_by(
            WordLemma.created_at.desc()
        ).limit(10).all()
        
        print(f"\nğŸ“ æœ€è¿‘æ·»åŠ çš„è¯æ±‡:")
        for word in recent_words:
            print(f"   - {word.lemma} ({word.pos or 'unknown'})")
        
        self.db.close()

    async def import_from_file(self, file_path: str):
        """ä»æ–‡ä»¶å¯¼å…¥è¯æ±‡åˆ—è¡¨"""
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                words = [line.strip() for line in f.readlines() if line.strip()]
            
            print(f"ğŸ“ ä»æ–‡ä»¶ {file_path} è¯»å–åˆ° {len(words)} ä¸ªè¯æ±‡")
            await self.batch_add_words(words)
            
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")


# ä¸€äº›å¸¸ç”¨è¯æ±‡åˆ—è¡¨ä¾›å‚è€ƒ
COMMON_GERMAN_WORDS = [
    # A1çº§åˆ«åŸºç¡€åŠ¨è¯
    "machen", "sagen", "kommen", "sehen", "wissen", "geben", "stehen", "lassen",
    "finden", "bleiben", "liegen", "fahren", "nehmen", "tun", "denken", "sprechen",
    
    # A1çº§åˆ«åŸºç¡€åè¯  
    "Mann", "Frau", "Kind", "Jahr", "Tag", "Hand", "Welt", "Leben", "Stadt",
    "Auge", "Frage", "Arbeit", "Ende", "Land", "Teil", "Weg", "Seite",
    
    # A1çº§åˆ«åŸºç¡€å½¢å®¹è¯
    "neu", "alt", "klein", "deutsch", "eigen", "recht", "social", "jung",
    "hoch", "lang", "wenig", "mÃ¶glich", "weit", "wichtig", "frÃ¼h", "politisch",
    
    # A2çº§åˆ«è¯æ±‡
    "verstehen", "bringen", "helfen", "zeigen", "lernen", "spielen", "leben",
    "kaufen", "essen", "trinken", "schlafen", "lachen", "weinen", "lieben"
]


async def main():
    """ä¸»å‡½æ•°"""
    
    if len(sys.argv) < 2:
        print("è¯æ±‡ç®¡ç†å·¥å…·ä½¿ç”¨æ–¹æ³•:")
        print("  python vocabulary_manager.py stats          # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯")
        print("  python vocabulary_manager.py add_common     # æ·»åŠ å¸¸ç”¨è¯æ±‡")
        print("  python vocabulary_manager.py import <file>  # ä»æ–‡ä»¶å¯¼å…¥")
        print("  python vocabulary_manager.py add <word1> <word2> ...  # æ·»åŠ æŒ‡å®šè¯æ±‡")
        return
    
    manager = VocabularyManager()
    command = sys.argv[1]
    
    if command == "stats":
        manager.show_stats()
        
    elif command == "add_common":
        await manager.batch_add_words(COMMON_GERMAN_WORDS)
        
    elif command == "import" and len(sys.argv) > 2:
        file_path = sys.argv[2]
        await manager.import_from_file(file_path)
        
    elif command == "add" and len(sys.argv) > 2:
        words = sys.argv[2:]
        await manager.batch_add_words(words)
        
    else:
        print("âŒ æ— æ•ˆçš„å‘½ä»¤")


if __name__ == "__main__":
    asyncio.run(main())