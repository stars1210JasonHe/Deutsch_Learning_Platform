#!/usr/bin/env python3
"""
å¢å¼ºæ•°æ®åº“ä¿®å¤å™¨ - ä¿®å¤æ‰€æœ‰è¯­æ³•ä¿¡æ¯
ä¿®å¤å†…å®¹:
1. ç¼ºå°‘çš„ä¸­æ–‡ç¿»è¯‘ (1781ä¸ªè¯æ±‡)
2. ç¼ºå°‘çš„ä¾‹å¥ (1013ä¸ªè¯æ±‡)  
3. ç¼ºå°‘çš„åè¯å† è¯ (878ä¸ªåè¯)
4. ç¼ºå°‘çš„åè¯å¤æ•° (2645ä¸ªåè¯)
5. ç¼ºå°‘çš„åŠ¨è¯å˜ä½è¡¨ (131ä¸ªåŠ¨è¯)
"""
import sqlite3
import asyncio
import json
import sys
import os
from datetime import datetime
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.getcwd())

try:
    from app.services.openai_service import OpenAIService
    from app.core.config import settings
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥åº”ç”¨æ¨¡å—: {e}")
    sys.exit(1)

class EnhancedDatabaseFixer:
    """å¢å¼ºæ•°æ®åº“ä¿®å¤å™¨"""
    
    def __init__(self, batch_size=20, delay=2.5):
        self.db_path = 'data/app.db'
        self.openai_service = OpenAIService()
        self.batch_size = batch_size
        self.delay = delay
        self.stats = {
            'words_processed': 0,
            'chinese_translations_added': 0,
            'examples_added': 0,
            'articles_added': 0,
            'plurals_added': 0,
            'verb_conjugations_added': 0,
            'errors': 0,
            'skipped': 0,
            'start_time': datetime.now()
        }
        
    async def generate_complete_noun_info(self, lemma, existing_notes=None):
        """ä¸ºåè¯ç”Ÿæˆå®Œæ•´ä¿¡æ¯ï¼ˆå† è¯ã€å¤æ•°ã€ä¸­æ–‡ç¿»è¯‘ã€ä¾‹å¥ï¼‰"""
        try:
            prompt = f"""
ä¸ºå¾·è¯­åè¯ "{lemma}" æä¾›å®Œæ•´çš„è¯­æ³•ä¿¡æ¯å’Œç¿»è¯‘ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›:
{{
    "article": "der/die/das",
    "plural": "å¤æ•°å½¢å¼",
    "translations_zh": ["ä¸­æ–‡ç¿»è¯‘1", "ä¸­æ–‡ç¿»è¯‘2"],
    "example": {{
        "de": "å¾·è¯­ä¾‹å¥ (ä½¿ç”¨è¯¥åè¯)",
        "en": "English example sentence",
        "zh": "ä¸­æ–‡ä¾‹å¥"
    }}
}}

è¦æ±‚:
1. å† è¯å¿…é¡»æ˜¯ der/die/das ä¹‹ä¸€
2. å¤æ•°å½¢å¼è¦å‡†ç¡®ï¼ˆå¦‚æœæ˜¯ä¸å¯æ•°åè¯ï¼Œè¿”å›åŸå½¢ï¼‰
3. ä¸­æ–‡ç¿»è¯‘ç®€æ´å‡†ç¡®
4. ä¾‹å¥è¦è‡ªç„¶å®ç”¨ï¼Œé•¿åº¦é€‚ä¸­

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
            
            response = await self.openai_service.client.chat.completions.create(
                model=self.openai_service.model,
                messages=[
                    {"role": "system", "content": "You are a precise German language assistant. Always respond with valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                max_tokens=300,
                temperature=0.6
            )
            
            if response and response.choices:
                content = response.choices[0].message.content.strip()
                data = json.loads(content)
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                required_fields = ['article', 'plural', 'translations_zh', 'example']
                if all(field in data for field in required_fields):
                    if data['article'] in ['der', 'die', 'das']:
                        return data
            
            return None
            
        except Exception as e:
            print(f"   âŒ ç”Ÿæˆåè¯ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def generate_complete_verb_info(self, lemma):
        """ä¸ºåŠ¨è¯ç”Ÿæˆå®Œæ•´ä¿¡æ¯ï¼ˆå˜ä½è¡¨ã€ä¸­æ–‡ç¿»è¯‘ã€ä¾‹å¥ï¼‰"""
        try:
            prompt = f"""
ä¸ºå¾·è¯­åŠ¨è¯ "{lemma}" æä¾›å®Œæ•´çš„è¯­æ³•ä¿¡æ¯å’Œç¿»è¯‘ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›:
{{
    "translations_zh": ["ä¸­æ–‡ç¿»è¯‘1", "ä¸­æ–‡ç¿»è¯‘2"],
    "conjugations": {{
        "praesens": {{
            "ich": "form",
            "du": "form", 
            "er_sie_es": "form",
            "wir": "form",
            "ihr": "form",
            "sie_Sie": "form"
        }},
        "praeteritum": {{
            "ich": "form",
            "du": "form",
            "er_sie_es": "form", 
            "wir": "form",
            "ihr": "form",
            "sie_Sie": "form"
        }},
        "perfekt": {{
            "ich": "bin/habe + partizip",
            "du": "bist/hast + partizip",
            "er_sie_es": "ist/hat + partizip",
            "wir": "sind/haben + partizip",
            "ihr": "seid/habt + partizip",
            "sie_Sie": "sind/haben + partizip"
        }},
        "plusquamperfekt": {{
            "ich": "war/hatte + partizip",
            "du": "warst/hattest + partizip",
            "er_sie_es": "war/hatte + partizip",
            "wir": "waren/hatten + partizip",
            "ihr": "wart/hattet + partizip",
            "sie_Sie": "waren/hatten + partizip"
        }},
        "futur_i": {{
            "ich": "werde + infinitiv",
            "du": "wirst + infinitiv",
            "er_sie_es": "wird + infinitiv",
            "wir": "werden + infinitiv",
            "ihr": "werdet + infinitiv",
            "sie_Sie": "werden + infinitiv"
        }},
        "konjunktiv_ii": {{
            "ich": "konjunktiv form",
            "du": "konjunktiv form",
            "er_sie_es": "konjunktiv form",
            "wir": "konjunktiv form",
            "ihr": "konjunktiv form",
            "sie_Sie": "konjunktiv form"
        }},
        "imperativ": {{
            "du": "command form",
            "ihr": "command form",
            "Sie": "command form"
        }}
    }},
    "example": {{
        "de": "å¾·è¯­ä¾‹å¥ (ä½¿ç”¨è¯¥åŠ¨è¯)",
        "en": "English example sentence",
        "zh": "ä¸­æ–‡ä¾‹å¥"
    }}
}}

è¦æ±‚:
1. åŠ¨è¯å˜ä½è¦å‡†ç¡®ï¼ŒåŒ…æ‹¬æ‰€æœ‰ä¸»è¦æ—¶æ€
2. åŒ…æ‹¬ç°åœ¨æ—¶ã€è¿‡å»æ—¶ã€å®Œæˆæ—¶ã€è¿‡å»å®Œæˆæ—¶ã€å°†æ¥æ—¶ã€è™šæ‹Ÿè¯­æ°”ã€å‘½ä»¤å¼
3. ä¸­æ–‡ç¿»è¯‘ç®€æ´å‡†ç¡®
4. ä¾‹å¥è¦è‡ªç„¶å®ç”¨
5. åŠ©åŠ¨è¯é€‰æ‹©è¦æ­£ç¡® (sein/haben)

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
            
            response = await self.openai_service.client.chat.completions.create(
                model=self.openai_service.model,
                messages=[
                    {"role": "system", "content": "You are a precise German language assistant. Always respond with valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                max_tokens=800,
                temperature=0.6
            )
            
            if response and response.choices:
                content = response.choices[0].message.content.strip()
                data = json.loads(content)
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                required_fields = ['translations_zh', 'conjugations', 'example']
                if all(field in data for field in required_fields):
                    if 'praesens' in data['conjugations']:
                        return data
            
            return None
            
        except Exception as e:
            print(f"   âŒ ç”ŸæˆåŠ¨è¯ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def save_noun_info_to_database(self, lemma_id, lemma, noun_data):
        """ä¿å­˜åè¯ä¿¡æ¯åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            success_count = 0
            
            # 1. æ›´æ–°noteså­—æ®µæ·»åŠ å† è¯å’Œå¤æ•°
            article = noun_data.get('article')
            plural = noun_data.get('plural')
            
            notes_parts = []
            if article:
                notes_parts.append(f"article:{article}")
            if plural and plural != lemma:  # é¿å…å¤æ•°å’ŒåŸå½¢ç›¸åŒæ—¶é‡å¤
                notes_parts.append(f"plural:{plural}")
            
            if notes_parts:
                notes = " ".join(notes_parts)
                cursor.execute("""
                    UPDATE word_lemmas 
                    SET notes = CASE 
                        WHEN notes IS NULL THEN ?
                        ELSE notes || ' ' || ?
                    END
                    WHERE id = ?
                """, (notes, notes, lemma_id))
                self.stats['articles_added'] += 1
                self.stats['plurals_added'] += 1
                success_count += 1
            
            # 2. æ·»åŠ ä¸­æ–‡ç¿»è¯‘
            translations_zh = noun_data.get('translations_zh', [])
            for translation in translations_zh:
                cursor.execute("""
                    INSERT INTO translations (lemma_id, lang_code, text, source)
                    VALUES (?, ?, ?, ?)
                """, (lemma_id, "zh", translation.strip(), "openai_enhanced_fix"))
                
            if translations_zh:
                self.stats['chinese_translations_added'] += len(translations_zh)
                success_count += 1
            
            # 3. æ·»åŠ ä¾‹å¥
            example = noun_data.get('example')
            if example and all(key in example for key in ['de', 'en', 'zh']):
                cursor.execute("""
                    INSERT INTO examples (lemma_id, de_text, en_text, zh_text)
                    VALUES (?, ?, ?, ?)
                """, (lemma_id, example['de'], example['en'], example['zh']))
                self.stats['examples_added'] += 1
                success_count += 1
            
            conn.commit()
            return success_count > 0
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜åè¯ä¿¡æ¯å¤±è´¥: {e}")
            conn.rollback()
            return False
            
        finally:
            conn.close()
    
    def save_verb_info_to_database(self, lemma_id, lemma, verb_data):
        """ä¿å­˜åŠ¨è¯ä¿¡æ¯åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            success_count = 0
            
            # 1. æ·»åŠ ä¸­æ–‡ç¿»è¯‘
            translations_zh = verb_data.get('translations_zh', [])
            for translation in translations_zh:
                cursor.execute("""
                    INSERT INTO translations (lemma_id, lang_code, text, source)
                    VALUES (?, ?, ?, ?)
                """, (lemma_id, "zh", translation.strip(), "openai_enhanced_fix"))
                
            if translations_zh:
                self.stats['chinese_translations_added'] += len(translations_zh)
                success_count += 1
            
            # 2. æ·»åŠ åŠ¨è¯å˜ä½
            conjugations = verb_data.get('conjugations', {})
            conjugation_count = 0
            
            for tense, persons in conjugations.items():
                if isinstance(persons, dict):
                    for person, form in persons.items():
                        if form:
                            cursor.execute("""
                                INSERT INTO word_forms (lemma_id, form, feature_key, feature_value)
                                VALUES (?, ?, ?, ?)
                            """, (lemma_id, form, "tense", f"{tense}_{person}"))
                            conjugation_count += 1
            
            if conjugation_count > 0:
                self.stats['verb_conjugations_added'] += conjugation_count
                success_count += 1
            
            # 3. æ·»åŠ ä¾‹å¥
            example = verb_data.get('example')
            if example and all(key in example for key in ['de', 'en', 'zh']):
                cursor.execute("""
                    INSERT INTO examples (lemma_id, de_text, en_text, zh_text)
                    VALUES (?, ?, ?, ?)
                """, (lemma_id, example['de'], example['en'], example['zh']))
                self.stats['examples_added'] += 1
                success_count += 1
            
            conn.commit()
            return success_count > 0
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜åŠ¨è¯ä¿¡æ¯å¤±è´¥: {e}")
            conn.rollback()
            return False
            
        finally:
            conn.close()
    
    def get_incomplete_nouns(self, limit=30):
        """è·å–ä¸å®Œæ•´çš„åè¯åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                SELECT wl.id, wl.lemma, wl.cefr, wl.frequency, wl.notes
                FROM word_lemmas wl
                WHERE wl.pos = 'noun'
                AND (
                    -- ç¼ºå°‘å† è¯
                    (wl.notes IS NULL OR wl.notes NOT LIKE '%article:%')
                    -- æˆ–ç¼ºå°‘ä¸­æ–‡ç¿»è¯‘
                    OR wl.id NOT IN (SELECT DISTINCT lemma_id FROM translations WHERE lang_code = 'zh')
                    -- æˆ–ç¼ºå°‘ä¾‹å¥
                    OR wl.id NOT IN (SELECT DISTINCT lemma_id FROM examples WHERE de_text IS NOT NULL)
                )
                ORDER BY 
                    CASE WHEN wl.cefr IN ('A1', 'A2') THEN 1 ELSE 2 END,
                    wl.frequency DESC NULLS LAST,
                    wl.lemma
                LIMIT ?
            """, (limit,))
            
            results = cursor.fetchall()
            print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªéœ€è¦ä¿®å¤çš„åè¯")
            return results
            
        finally:
            conn.close()
    
    def get_incomplete_verbs(self, limit=20):
        """è·å–ä¸å®Œæ•´çš„åŠ¨è¯åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                SELECT wl.id, wl.lemma, wl.cefr, wl.frequency
                FROM word_lemmas wl
                WHERE wl.pos = 'verb'
                AND (
                    -- ç¼ºå°‘å˜ä½
                    wl.id NOT IN (SELECT DISTINCT lemma_id FROM word_forms)
                    -- æˆ–ç¼ºå°‘ä¸­æ–‡ç¿»è¯‘
                    OR wl.id NOT IN (SELECT DISTINCT lemma_id FROM translations WHERE lang_code = 'zh')
                    -- æˆ–ç¼ºå°‘ä¾‹å¥
                    OR wl.id NOT IN (SELECT DISTINCT lemma_id FROM examples WHERE de_text IS NOT NULL)
                )
                ORDER BY 
                    CASE WHEN wl.cefr IN ('A1', 'A2') THEN 1 ELSE 2 END,
                    wl.frequency DESC NULLS LAST,
                    wl.lemma
                LIMIT ?
            """, (limit,))
            
            results = cursor.fetchall()
            print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªéœ€è¦ä¿®å¤çš„åŠ¨è¯")
            return results
            
        finally:
            conn.close()
    
    async def fix_incomplete_nouns(self, limit=30):
        """ä¿®å¤ä¸å®Œæ•´çš„åè¯"""
        print("ğŸ”„ ä¿®å¤ä¸å®Œæ•´åè¯")
        print("=" * 50)
        
        nouns_to_fix = self.get_incomplete_nouns(limit)
        
        if not nouns_to_fix:
            print("âœ… æ²¡æœ‰éœ€è¦ä¿®å¤çš„åè¯")
            return
        
        print(f"ğŸ“ å°†å¤„ç† {len(nouns_to_fix)} ä¸ªåè¯...")
        
        for i, (lemma_id, lemma, cefr, frequency, notes) in enumerate(nouns_to_fix, 1):
            cefr_str = f" [{cefr}]" if cefr else ""
            freq_str = f" (é¢‘ç‡:{frequency})" if frequency else ""
            print(f"[{i}/{len(nouns_to_fix)}] å¤„ç†: {lemma}{cefr_str}{freq_str}")
            
            self.stats['words_processed'] += 1
            
            try:
                # ç”Ÿæˆå®Œæ•´åè¯ä¿¡æ¯
                noun_data = await self.generate_complete_noun_info(lemma, notes)
                
                if noun_data:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    if self.save_noun_info_to_database(lemma_id, lemma, noun_data):
                        article = noun_data.get('article', '?')
                        plural = noun_data.get('plural', '?')
                        print(f"   âœ… {article} {lemma}, å¤æ•°: {plural}")
                    else:
                        self.stats['errors'] += 1
                else:
                    print(f"   âŒ æœªèƒ½ç”Ÿæˆåè¯ä¿¡æ¯")
                    self.stats['errors'] += 1
                
                # å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i < len(nouns_to_fix):
                    await asyncio.sleep(self.delay)
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                self.stats['errors'] += 1
    
    async def fix_incomplete_verbs(self, limit=20):
        """ä¿®å¤ä¸å®Œæ•´çš„åŠ¨è¯"""
        print("ğŸ”„ ä¿®å¤ä¸å®Œæ•´åŠ¨è¯")
        print("=" * 50)
        
        verbs_to_fix = self.get_incomplete_verbs(limit)
        
        if not verbs_to_fix:
            print("âœ… æ²¡æœ‰éœ€è¦ä¿®å¤çš„åŠ¨è¯")
            return
        
        print(f"ğŸ“ å°†å¤„ç† {len(verbs_to_fix)} ä¸ªåŠ¨è¯...")
        
        for i, (lemma_id, lemma, cefr, frequency) in enumerate(verbs_to_fix, 1):
            cefr_str = f" [{cefr}]" if cefr else ""
            freq_str = f" (é¢‘ç‡:{frequency})" if frequency else ""
            print(f"[{i}/{len(verbs_to_fix)}] å¤„ç†: {lemma}{cefr_str}{freq_str}")
            
            self.stats['words_processed'] += 1
            
            try:
                # ç”Ÿæˆå®Œæ•´åŠ¨è¯ä¿¡æ¯
                verb_data = await self.generate_complete_verb_info(lemma)
                
                if verb_data:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    if self.save_verb_info_to_database(lemma_id, lemma, verb_data):
                        conjugations = verb_data.get('conjugations', {})
                        praesens = conjugations.get('praesens', {})
                        ich_form = praesens.get('ich', '?')
                        print(f"   âœ… {lemma} - ich {ich_form}, ç­‰...")
                    else:
                        self.stats['errors'] += 1
                else:
                    print(f"   âŒ æœªèƒ½ç”ŸæˆåŠ¨è¯ä¿¡æ¯")
                    self.stats['errors'] += 1
                
                # å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i < len(verbs_to_fix):
                    await asyncio.sleep(self.delay)
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                self.stats['errors'] += 1
    
    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        elapsed = datetime.now() - self.stats['start_time']
        
        print(f"\nğŸ‰ å¢å¼ºæ•°æ®åº“ä¿®å¤å®Œæˆ!")
        print("=" * 60)
        print(f"å¤„ç†è¯æ±‡æ€»æ•°: {self.stats['words_processed']}")
        print(f"æ·»åŠ ä¸­æ–‡ç¿»è¯‘: {self.stats['chinese_translations_added']}")
        print(f"æ·»åŠ ä¾‹å¥: {self.stats['examples_added']}")
        print(f"æ·»åŠ å† è¯: {self.stats['articles_added']}")
        print(f"æ·»åŠ å¤æ•°: {self.stats['plurals_added']}")
        print(f"æ·»åŠ åŠ¨è¯å˜ä½: {self.stats['verb_conjugations_added']}")
        print(f"é”™è¯¯æ•°é‡: {self.stats['errors']}")
        print(f"æ€»ç”¨æ—¶: {elapsed}")
        
        total_improvements = (self.stats['chinese_translations_added'] + 
                            self.stats['examples_added'] + 
                            self.stats['articles_added'] + 
                            self.stats['plurals_added'] + 
                            self.stats['verb_conjugations_added'])
        
        print(f"æ€»æ”¹è¿›é¡¹ç›®: {total_improvements}")
        
        if total_improvements > 0:
            print(f"\nâœ… ä¿®å¤æˆåŠŸ! ç”¨æˆ·ç°åœ¨å¯ä»¥åœ¨å‰ç«¯çœ‹åˆ°:")
            print("   â€¢ åè¯çš„å† è¯ (der/die/das)")
            print("   â€¢ åè¯çš„å¤æ•°å½¢å¼") 
            print("   â€¢ åŠ¨è¯çš„å˜ä½è¡¨ (ç°åœ¨æ—¶/è¿‡å»æ—¶)")
            print("   â€¢ æ›´å¤šä¸­æ–‡ç¿»è¯‘")
            print("   â€¢ æ›´å¤šä¾‹å¥")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºæ•°æ®åº“ä¿®å¤å™¨ - ä¿®å¤æ‰€æœ‰è¯­æ³•ä¿¡æ¯')
    parser.add_argument('--mode', 
                       choices=['nouns', 'verbs', 'all'], 
                       default='all',
                       help='ä¿®å¤æ¨¡å¼ (é»˜è®¤: all)')
    parser.add_argument('--noun-limit', type=int, default=20, 
                       help='åè¯å¤„ç†é™åˆ¶ (é»˜è®¤: 20)')
    parser.add_argument('--verb-limit', type=int, default=15,
                       help='åŠ¨è¯å¤„ç†é™åˆ¶ (é»˜è®¤: 15)')
    parser.add_argument('--delay', type=float, default=3.0,
                       help='APIè°ƒç”¨å»¶è¿Ÿ(ç§’) (é»˜è®¤: 3.0)')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¢å¼ºæ•°æ®åº“ä¿®å¤å™¨")
    print("=" * 60)
    print("ä¿®å¤å†…å®¹: å† è¯ã€å¤æ•°ã€åŠ¨è¯å˜ä½ã€ä¸­æ–‡ç¿»è¯‘ã€ä¾‹å¥")
    print(f"âš™ï¸  é…ç½®:")
    print(f"   ä¿®å¤æ¨¡å¼: {args.mode}")
    print(f"   åè¯é™åˆ¶: {args.noun_limit}")
    print(f"   åŠ¨è¯é™åˆ¶: {args.verb_limit}")
    print(f"   APIå»¶è¿Ÿ: {args.delay}ç§’")
    
    # æ£€æŸ¥OpenAIé…ç½®
    if not settings.openai_api_key:
        print("\nâŒ é”™è¯¯: æœªé…ç½®OpenAI APIå¯†é’¥")
        return
    
    print(f"\nâœ… OpenAIé…ç½®æ­£å¸¸")
    
    fixer = EnhancedDatabaseFixer(delay=args.delay)
    
    try:
        if args.mode in ['nouns', 'all']:
            await fixer.fix_incomplete_nouns(args.noun_limit)
            
        if args.mode in ['verbs', 'all']:
            await fixer.fix_incomplete_verbs(args.verb_limit)
        
        fixer.print_final_stats()
        
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("1. æµ‹è¯•å‰ç«¯æœç´¢åŠŸèƒ½")
        print("2. éªŒè¯è¯­æ³•ä¿¡æ¯æ˜¾ç¤º")
        print("3. æ ¹æ®éœ€è¦ç»§ç»­ä¿®å¤æ›´å¤šè¯æ±‡")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ä¿®å¤è¢«ç”¨æˆ·ä¸­æ–­")
        fixer.print_final_stats()
    except Exception as e:
        print(f"\nâŒ ä¿®å¤å¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(main())