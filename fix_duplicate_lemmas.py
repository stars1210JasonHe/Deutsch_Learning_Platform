#!/usr/bin/env python3
"""
ä¿®å¤é‡å¤è¯æ¡é—®é¢˜
å¤„ç†ç±»ä¼¼æƒ…å†µï¼š
- sehen (ä¸å®šå¼) å’Œ sehe (ç¬¬ä¸€äººç§°) åº”è¯¥æŒ‡å‘åŒä¸€ä¸ªè¯æ¡
- Zug (å•æ•°) å’Œ ZÃ¼ge (å¤æ•°) åº”è¯¥æŒ‡å‘åŒä¸€ä¸ªè¯æ¡
- åŠ¨è¯å˜ä½å½¢å¼åº”è¯¥å­˜åœ¨word_formsè¡¨ä¸­ï¼Œè€Œä¸æ˜¯å•ç‹¬çš„lemmas
"""
import sqlite3
import re
from datetime import datetime
from collections import defaultdict

class DuplicateLemmaFixer:
    
    def __init__(self):
        self.db_path = 'data/app.db'
        self.stats = {
            'lemmas_merged': 0,
            'forms_moved': 0,
            'duplicates_found': 0,
            'start_time': datetime.now()
        }
    
    def analyze_duplicates(self):
        """åˆ†æé‡å¤çš„è¯æ¡"""
        print("ğŸ” åˆ†æé‡å¤è¯æ¡")
        print("=" * 50)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # 1. æŸ¥æ‰¾å¯èƒ½çš„åŠ¨è¯é‡å¤ï¼ˆåŸºäºç›¸ä¼¼æ€§ï¼‰
        cursor.execute("""
            SELECT lemma, pos, id, cefr, frequency 
            FROM word_lemmas 
            WHERE pos = 'verb' 
            ORDER BY lemma
        """)
        verbs = cursor.fetchall()
        
        verb_groups = self.group_similar_verbs(verbs)
        
        print(f"ğŸ“Š å‘ç° {len(verb_groups)} ç»„å¯èƒ½é‡å¤çš„åŠ¨è¯:")
        for i, (base_verb, variants) in enumerate(verb_groups.items(), 1):
            if len(variants) > 1:
                print(f"  {i}. {base_verb} ç»„:")
                for lemma, pos, lemma_id, cefr, freq in variants:
                    cefr_str = f"[{cefr}]" if cefr else ""
                    freq_str = f"(é¢‘ç‡:{freq})" if freq else ""
                    print(f"     â€¢ {lemma} {cefr_str} {freq_str} (ID: {lemma_id})")
        
        # 2. æŸ¥æ‰¾å¯èƒ½çš„åè¯é‡å¤ï¼ˆå•å¤æ•°ï¼‰
        cursor.execute("""
            SELECT lemma, pos, id, cefr, frequency 
            FROM word_lemmas 
            WHERE pos = 'noun' 
            ORDER BY lemma
        """)
        nouns = cursor.fetchall()
        
        noun_groups = self.group_singular_plural_nouns(nouns)
        
        print(f"\nğŸ“Š å‘ç° {len(noun_groups)} ç»„å¯èƒ½é‡å¤çš„åè¯:")
        for i, (base_noun, variants) in enumerate(noun_groups.items(), 1):
            if len(variants) > 1:
                print(f"  {i}. {base_noun} ç»„:")
                for lemma, pos, lemma_id, cefr, freq in variants:
                    cefr_str = f"[{cefr}]" if cefr else ""
                    freq_str = f"(é¢‘ç‡:{freq})" if freq else ""
                    print(f"     â€¢ {lemma} {cefr_str} {freq_str} (ID: {lemma_id})")
        
        conn.close()
        return verb_groups, noun_groups
    
    def group_similar_verbs(self, verbs):
        """å°†ç›¸ä¼¼çš„åŠ¨è¯åˆ†ç»„"""
        groups = defaultdict(list)
        
        for verb_data in verbs:
            lemma = verb_data[0]
            
            # æ‰¾åˆ°å¯èƒ½çš„åŸºç¡€å½¢å¼
            base_form = self.get_verb_base_form(lemma)
            groups[base_form].append(verb_data)
        
        # åªè¿”å›æœ‰å¤šä¸ªå˜ä½“çš„ç»„
        return {k: v for k, v in groups.items() if len(v) > 1}
    
    def get_verb_base_form(self, verb):
        """æ¨æ–­åŠ¨è¯çš„åŸºç¡€å½¢å¼ï¼ˆä¸å®šå¼ï¼‰"""
        # å¸¸è§å¾·è¯­åŠ¨è¯å˜ä½è§„å¾‹
        verb = verb.lower()
        
        # å¦‚æœä»¥-enç»“å°¾ï¼Œå¯èƒ½æ˜¯ä¸å®šå¼
        if verb.endswith('en'):
            return verb
        
        # ä¸€äº›å¸¸è§çš„å˜ä½å½¢å¼è½¬æ¢
        conversions = {
            # å¼ºå˜ä½åŠ¨è¯
            'bin': 'sein',
            'bist': 'sein', 
            'ist': 'sein',
            'sind': 'sein',
            'seid': 'sein',
            'war': 'sein',
            'warst': 'sein',
            'waren': 'sein',
            'wart': 'sein',
            
            'habe': 'haben',
            'hast': 'haben',
            'hat': 'haben',
            'hatte': 'haben',
            'hattest': 'haben',
            'hatten': 'haben',
            'hattet': 'haben',
            
            'sehe': 'sehen',
            'siehst': 'sehen',
            'sieht': 'sehen',
            'sah': 'sehen',
            'sahst': 'sehen',
            'sahen': 'sehen',
            'saht': 'sehen',
            
            'gehe': 'gehen',
            'gehst': 'gehen',
            'geht': 'gehen',
            'ging': 'gehen',
            'gingst': 'gehen',
            'gingen': 'gehen',
            'gingt': 'gehen',
            
            'komme': 'kommen',
            'kommst': 'kommen',
            'kommt': 'kommen',
            'kam': 'kommen',
            'kamst': 'kommen',
            'kamen': 'kommen',
            'kamt': 'kommen',
        }
        
        if verb in conversions:
            return conversions[verb]
        
        # è§„å¾‹å˜ä½ï¼šå»æ‰äººç§°è¯å°¾ï¼ŒåŠ -en
        # ç¬¬ä¸€äººç§°é€šå¸¸å»-eåŠ -en
        if verb.endswith('e') and len(verb) > 2:
            base = verb[:-1] + 'en'
            return base
        
        # å…¶ä»–å½¢å¼å°è¯•é‡æ„
        if verb.endswith(('st', 't')):  # du/erå½¢å¼
            # è¿™æ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶è¿”å›åŸå½¢
            pass
        
        return verb
    
    def group_singular_plural_nouns(self, nouns):
        """å°†å•å¤æ•°åè¯åˆ†ç»„"""
        groups = defaultdict(list)
        
        for noun_data in nouns:
            lemma = noun_data[0]
            
            # æ‰¾åˆ°å¯èƒ½çš„å•æ•°å½¢å¼
            singular_form = self.get_singular_form(lemma)
            groups[singular_form].append(noun_data)
        
        return {k: v for k, v in groups.items() if len(v) > 1}
    
    def get_singular_form(self, noun):
        """æ¨æ–­åè¯çš„å•æ•°å½¢å¼"""
        noun_lower = noun.lower()
        
        # å¸¸è§å¤æ•°è§„å¾‹
        plural_patterns = [
            # -eå¤æ•°
            ('zÃ¼ge', 'zug'),
            ('tage', 'tag'),
            ('jahre', 'jahr'),
            
            # -erå¤æ•°  
            ('kinder', 'kind'),
            ('mÃ¤nner', 'mann'),
            ('hÃ¤user', 'haus'),
            
            # -enå¤æ•°
            ('frauen', 'frau'),
            ('straÃŸen', 'straÃŸe'),
            
            # -så¤æ•°
            ('autos', 'auto'),
            ('bÃ¼ros', 'bÃ¼ro'),
        ]
        
        for plural, singular in plural_patterns:
            if noun_lower == plural:
                return singular
        
        # ä¸€èˆ¬è§„å¾‹ï¼šå¦‚æœä»¥å¤æ•°æ ‡è®°ç»“å°¾ï¼Œå°è¯•å»æ‰
        if noun_lower.endswith('e') and len(noun_lower) > 2:
            singular_candidate = noun_lower[:-1]
            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆç†çš„å•æ•°å½¢å¼
            return singular_candidate
        
        return noun_lower
    
    def merge_duplicate_lemmas(self, verb_groups, noun_groups):
        """åˆå¹¶é‡å¤çš„è¯æ¡"""
        print(f"\nğŸ”§ å¼€å§‹åˆå¹¶é‡å¤è¯æ¡")
        print("=" * 50)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # åˆå¹¶åŠ¨è¯ç»„
            for base_verb, variants in verb_groups.items():
                if len(variants) > 1:
                    self.merge_verb_group(cursor, base_verb, variants)
            
            # åˆå¹¶åè¯ç»„  
            for base_noun, variants in noun_groups.items():
                if len(variants) > 1:
                    self.merge_noun_group(cursor, base_noun, variants)
            
            conn.commit()
            print(f"\nâœ… åˆå¹¶å®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
            conn.rollback()
        finally:
            conn.close()
    
    def merge_verb_group(self, cursor, base_verb, variants):
        """åˆå¹¶ä¸€ç»„åŠ¨è¯å˜ä½“"""
        # æ‰¾åˆ°æœ€å¥½çš„ä¸»è¯æ¡ï¼ˆä¼˜å…ˆçº§ï¼šé¢‘ç‡ > CEFRçº§åˆ« > æœ€é•¿çš„è¯ï¼‰
        main_lemma = self.select_main_lemma(variants)
        other_lemmas = [v for v in variants if v[2] != main_lemma[2]]  # v[2] is id
        
        print(f"ğŸ“ åˆå¹¶åŠ¨è¯ç»„ '{base_verb}' -> ä¸»è¯æ¡: {main_lemma[0]} (ID: {main_lemma[2]})")
        
        for other_lemma in other_lemmas:
            lemma_text, pos, lemma_id, cefr, freq = other_lemma
            print(f"   ğŸ”„ å°† '{lemma_text}' è½¬ä¸ºè¯å½¢...")
            
            # å°†å…¶ä»–è¯æ¡è½¬ä¸ºword_forms
            self.convert_lemma_to_word_form(cursor, lemma_id, main_lemma[2], lemma_text)
            
            # è¿ç§»ç›¸å…³æ•°æ®
            self.migrate_lemma_data(cursor, lemma_id, main_lemma[2])
            
            # åˆ é™¤åŸè¯æ¡
            cursor.execute("DELETE FROM word_lemmas WHERE id = ?", (lemma_id,))
            
            self.stats['lemmas_merged'] += 1
            self.stats['forms_moved'] += 1
    
    def merge_noun_group(self, cursor, base_noun, variants):
        """åˆå¹¶ä¸€ç»„åè¯å˜ä½“ï¼ˆå•å¤æ•°ï¼‰"""
        main_lemma = self.select_main_lemma(variants)
        other_lemmas = [v for v in variants if v[2] != main_lemma[2]]
        
        print(f"ğŸ“ åˆå¹¶åè¯ç»„ '{base_noun}' -> ä¸»è¯æ¡: {main_lemma[0]} (ID: {main_lemma[2]})")
        
        for other_lemma in other_lemmas:
            lemma_text, pos, lemma_id, cefr, freq = other_lemma
            print(f"   ğŸ”„ å°† '{lemma_text}' ä¿¡æ¯åˆå¹¶åˆ°ä¸»è¯æ¡...")
            
            # å¯¹äºåè¯ï¼Œå¦‚æœæ˜¯å¤æ•°å½¢å¼ï¼Œæ›´æ–°ä¸»è¯æ¡çš„notesæ·»åŠ å¤æ•°ä¿¡æ¯
            if lemma_text.lower() != main_lemma[0].lower():
                self.update_plural_info(cursor, main_lemma[2], lemma_text)
            
            # è¿ç§»ç›¸å…³æ•°æ®
            self.migrate_lemma_data(cursor, lemma_id, main_lemma[2])
            
            # åˆ é™¤åŸè¯æ¡
            cursor.execute("DELETE FROM word_lemmas WHERE id = ?", (lemma_id,))
            
            self.stats['lemmas_merged'] += 1
    
    def select_main_lemma(self, variants):
        """é€‰æ‹©æœ€ä½³çš„ä¸»è¯æ¡"""
        # æ’åºè§„åˆ™ï¼š1. é¢‘ç‡é«˜ 2. CEFRçº§åˆ«ä½ 3. è¯é•¿ï¼ˆä¸å®šå¼é€šå¸¸æ›´é•¿ï¼‰
        def sort_key(variant):
            lemma, pos, lemma_id, cefr, freq = variant
            
            # é¢‘ç‡æƒé‡
            freq_score = freq if freq else 0
            
            # CEFRæƒé‡ (A1=4, A2=3, B1=2, B2=1, å…¶ä»–=0)
            cefr_score = {'A1': 4, 'A2': 3, 'B1': 2, 'B2': 1}.get(cefr, 0)
            
            # è¯é•¿æƒé‡ï¼ˆä¸å®šå¼é€šå¸¸æ›´é•¿ï¼‰
            length_score = len(lemma)
            
            return (freq_score, cefr_score, length_score)
        
        return max(variants, key=sort_key)
    
    def convert_lemma_to_word_form(self, cursor, old_lemma_id, new_lemma_id, form_text):
        """å°†è¯æ¡è½¬æ¢ä¸ºè¯å½¢"""
        cursor.execute("""
            INSERT INTO word_forms (lemma_id, form, feature_key, feature_value)
            VALUES (?, ?, ?, ?)
        """, (new_lemma_id, form_text, "variant", "lemma_variant"))
        
    def update_plural_info(self, cursor, main_lemma_id, plural_form):
        """æ›´æ–°ä¸»è¯æ¡çš„å¤æ•°ä¿¡æ¯"""
        cursor.execute("""
            UPDATE word_lemmas 
            SET notes = CASE 
                WHEN notes IS NULL THEN ? 
                WHEN notes NOT LIKE '%plural:%' THEN notes || ' ' || ?
                ELSE notes
            END
            WHERE id = ?
        """, (f"plural:{plural_form}", f"plural:{plural_form}", main_lemma_id))
    
    def migrate_lemma_data(self, cursor, old_lemma_id, new_lemma_id):
        """è¿ç§»è¯æ¡ç›¸å…³çš„æ‰€æœ‰æ•°æ®"""
        
        # è¿ç§»ç¿»è¯‘æ•°æ®ï¼ˆé¿å…é‡å¤ï¼‰
        cursor.execute("""
            INSERT OR IGNORE INTO translations (lemma_id, lang_code, text, source)
            SELECT ?, lang_code, text, source 
            FROM translations 
            WHERE lemma_id = ?
        """, (new_lemma_id, old_lemma_id))
        
        cursor.execute("DELETE FROM translations WHERE lemma_id = ?", (old_lemma_id,))
        
        # è¿ç§»ä¾‹å¥æ•°æ®
        cursor.execute("""
            INSERT OR IGNORE INTO examples (lemma_id, de_text, en_text, zh_text, level)
            SELECT ?, de_text, en_text, zh_text, level
            FROM examples 
            WHERE lemma_id = ?
        """, (new_lemma_id, old_lemma_id))
        
        cursor.execute("DELETE FROM examples WHERE lemma_id = ?", (old_lemma_id,))
        
        # è¿ç§»è¯å½¢æ•°æ®
        cursor.execute("""
            UPDATE word_forms 
            SET lemma_id = ? 
            WHERE lemma_id = ?
        """, (new_lemma_id, old_lemma_id))
    
    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        elapsed = datetime.now() - self.stats['start_time']
        
        print(f"\nğŸ‰ é‡å¤è¯æ¡ä¿®å¤å®Œæˆ!")
        print("=" * 50)
        print(f"åˆå¹¶çš„è¯æ¡æ•°: {self.stats['lemmas_merged']}")
        print(f"è½¬æ¢çš„è¯å½¢æ•°: {self.stats['forms_moved']}")
        print(f"æ€»ç”¨æ—¶: {elapsed}")
        
        if self.stats['lemmas_merged'] > 0:
            print(f"\nâœ… ä¿®å¤æˆåŠŸ! ç°åœ¨:")
            print("   â€¢ åŠ¨è¯å˜ä½å½¢å¼æŒ‡å‘æ­£ç¡®çš„ä¸å®šå¼è¯æ¡")
            print("   â€¢ åè¯å•å¤æ•°å½¢å¼å·²åˆå¹¶")
            print("   â€¢ æ•°æ®åº“ç»“æ„æ›´åŠ æ¸…æ™°")

def main():
    print("ğŸš€ é‡å¤è¯æ¡ä¿®å¤å·¥å…·")
    print("=" * 60)
    print("å°†åˆå¹¶é‡å¤çš„è¯æ¡ï¼š")
    print("â€¢ åŠ¨è¯å˜ä½å½¢å¼ (sehe -> sehen)")  
    print("â€¢ åè¯å•å¤æ•° (ZÃ¼ge -> Zug)")
    print()
    
    fixer = DuplicateLemmaFixer()
    
    # åˆ†æé‡å¤è¯æ¡
    verb_groups, noun_groups = fixer.analyze_duplicates()
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­åˆå¹¶
    print(f"\nğŸ’¡ å‘ç°é‡å¤è¯æ¡ç»„:")
    print(f"   åŠ¨è¯ç»„: {sum(1 for v in verb_groups.values() if len(v) > 1)}")
    print(f"   åè¯ç»„: {sum(1 for v in noun_groups.values() if len(v) > 1)}")
    
    response = input("\næ˜¯å¦ç»§ç»­åˆå¹¶ï¼Ÿ(y/n): ").lower().strip()
    
    if response == 'y':
        # æ‰§è¡Œåˆå¹¶
        fixer.merge_duplicate_lemmas(verb_groups, noun_groups)
        fixer.print_final_stats()
    else:
        print("å–æ¶ˆåˆå¹¶æ“ä½œã€‚")

if __name__ == "__main__":
    main()