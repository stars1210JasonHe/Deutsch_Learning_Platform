#!/usr/bin/env python3
"""
å¾·è¯­è¯åº“æ‰¹é‡å›å¡«è„šæœ¬
ä½¿ç”¨OpenAIè¡¥å…¨ç°æœ‰è¯æ¡çš„ç¼ºå¤±å­—æ®µ
"""
import asyncio
import sys
import os
import sqlite3
import json
import argparse
from typing import List, Dict, Any, Optional
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(" - Copy.env")
sys.path.append(os.getcwd())

from app.services.lexicon_llm_service import LexiconLLMService

class LexiconBackfillService:
    """è¯åº“å›å¡«æœåŠ¡"""
    
    def __init__(self):
        self.llm_service = LexiconLLMService()
        self.db_path = 'data/app.db'
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed': 0,
            'enriched_nouns': 0,
            'enriched_verbs': 0,
            'enriched_adjectives': 0,
            'failed': 0,
            'skipped': 0,
            'start_time': datetime.now()
        }
    
    def get_incomplete_lemmas(self, limit: int = None, offset: int = 0) -> List[Dict[str, Any]]:
        """è·å–éœ€è¦è¡¥å…¨çš„è¯æ¡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æŸ¥æ‰¾ç¼ºå°‘é‡è¦å­—æ®µçš„è¯æ¡
            query = """
                SELECT 
                    wl.id as lemma_id,
                    wl.lemma,
                    wl.pos,
                    ls.id as sense_id,
                    ls.upos,
                    ls.gender,
                    ls.gloss_en,
                    ls.gloss_zh,
                    np.gen_sg,
                    np.plural,
                    vp.aux,
                    vp.partizip_ii,
                    COUNT(fu.id) as form_count
                FROM word_lemmas wl
                JOIN lemma_senses ls ON ls.lemma_id = wl.id
                LEFT JOIN noun_props np ON np.sense_id = ls.id
                LEFT JOIN verb_props vp ON vp.sense_id = ls.id
                LEFT JOIN forms_unimorph fu ON fu.sense_id = ls.id
                WHERE 
                    -- åè¯ç¼ºå°‘æ€§åˆ«æˆ–å¤æ•°
                    (ls.upos = 'NOUN' AND (ls.gender IS NULL OR np.plural IS NULL))
                    OR
                    -- åŠ¨è¯ç¼ºå°‘åŠ©åŠ¨è¯æˆ–è¿‡å»åˆ†è¯
                    (ls.upos = 'VERB' AND (vp.aux IS NULL OR vp.partizip_ii IS NULL))
                    OR
                    -- ç¼ºå°‘è‹±æ–‡æˆ–ä¸­æ–‡é‡Šä¹‰
                    (ls.gloss_en IS NULL OR ls.gloss_zh IS NULL)
                    OR
                    -- ç¼ºå°‘è¯å½¢å˜åŒ–
                    (fu.id IS NULL)
                GROUP BY wl.id, ls.id
                ORDER BY 
                    CASE wl.pos 
                        WHEN 'noun' THEN 1
                        WHEN 'verb' THEN 2
                        WHEN 'adjective' THEN 3
                        ELSE 4
                    END,
                    wl.lemma
            """
            
            if limit:
                query += f" LIMIT {limit} OFFSET {offset}"
            
            cursor.execute(query)
            columns = [desc[0] for desc in cursor.description]
            
            results = []
            for row in cursor.fetchall():
                results.append(dict(zip(columns, row)))
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªéœ€è¦è¡¥å…¨çš„è¯æ¡")
            return results
            
        finally:
            conn.close()
    
    async def enrich_lemma(self, lemma_data: Dict[str, Any]) -> bool:
        """è¡¥å…¨å•ä¸ªè¯æ¡"""
        lemma = lemma_data['lemma']
        upos = lemma_data['upos']
        sense_id = lemma_data['sense_id']
        
        try:
            print(f"\nğŸ” å¤„ç†: {lemma} ({upos})")
            
            if upos == 'NOUN':
                return await self._enrich_noun(lemma_data)
            elif upos == 'VERB':
                return await self._enrich_verb(lemma_data)
            elif upos == 'ADJ':
                return await self._enrich_adjective(lemma_data)
            else:
                print(f"   âš ï¸ è·³è¿‡ä¸æ”¯æŒçš„è¯æ€§: {upos}")
                self.stats['skipped'] += 1
                return False
                
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
            self.stats['failed'] += 1
            return False
        finally:
            self.stats['processed'] += 1
    
    async def _enrich_noun(self, lemma_data: Dict[str, Any]) -> bool:
        """è¡¥å…¨åè¯"""
        lemma = lemma_data['lemma']
        sense_id = lemma_data['sense_id']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å…¨
        needs_enrichment = (
            not lemma_data['gender'] or 
            not lemma_data['plural'] or
            not lemma_data['gloss_en'] or
            not lemma_data['gloss_zh'] or
            lemma_data['form_count'] == 0
        )
        
        if not needs_enrichment:
            print(f"   âœ… {lemma} å·²å®Œæ•´ï¼Œè·³è¿‡")
            self.stats['skipped'] += 1
            return True
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = {}
        if lemma_data.get('gender'):
            context['existing_gender'] = lemma_data['gender']
        
        # è°ƒç”¨LLMè·å–è¡¥å…¨æ•°æ®
        print(f"   ğŸ¤– è°ƒç”¨LLMè¡¥å…¨åè¯æ•°æ®...")
        enrichment_data = await self.llm_service.enrich_noun(lemma, context)
        
        if not enrichment_data:
            print(f"   âŒ LLMè¿”å›ç©ºæ•°æ®")
            return False
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        return await self._save_noun_data(sense_id, enrichment_data)
    
    async def _enrich_verb(self, lemma_data: Dict[str, Any]) -> bool:
        """è¡¥å…¨åŠ¨è¯"""
        lemma = lemma_data['lemma']
        sense_id = lemma_data['sense_id']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å…¨
        needs_enrichment = (
            not lemma_data['aux'] or
            not lemma_data['partizip_ii'] or
            not lemma_data['gloss_en'] or
            not lemma_data['gloss_zh'] or
            lemma_data['form_count'] == 0
        )
        
        if not needs_enrichment:
            print(f"   âœ… {lemma} å·²å®Œæ•´ï¼Œè·³è¿‡")
            self.stats['skipped'] += 1
            return True
        
        # è°ƒç”¨LLMè·å–è¡¥å…¨æ•°æ®
        print(f"   ğŸ¤– è°ƒç”¨LLMè¡¥å…¨åŠ¨è¯æ•°æ®...")
        enrichment_data = await self.llm_service.enrich_verb(lemma)
        
        if not enrichment_data:
            print(f"   âŒ LLMè¿”å›ç©ºæ•°æ®")
            return False
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        return await self._save_verb_data(sense_id, enrichment_data)
    
    async def _enrich_adjective(self, lemma_data: Dict[str, Any]) -> bool:
        """è¡¥å…¨å½¢å®¹è¯"""
        lemma = lemma_data['lemma']
        sense_id = lemma_data['sense_id']
        
        # è°ƒç”¨LLMè·å–è¡¥å…¨æ•°æ®
        print(f"   ğŸ¤– è°ƒç”¨LLMè¡¥å…¨å½¢å®¹è¯æ•°æ®...")
        enrichment_data = await self.llm_service.enrich_adjective(lemma)
        
        if not enrichment_data:
            print(f"   âŒ LLMè¿”å›ç©ºæ•°æ®")
            return False
        
        # ä¿å­˜åŸºç¡€æ•°æ®
        return await self._save_adjective_data(sense_id, enrichment_data)
    
    async def _save_noun_data(self, sense_id: int, data: Dict[str, Any]) -> bool:
        """ä¿å­˜åè¯æ•°æ®åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æ›´æ–° lemma_senses
            cursor.execute("""
                UPDATE lemma_senses 
                SET gender = COALESCE(gender, ?),
                    gloss_en = COALESCE(gloss_en, ?),
                    gloss_zh = COALESCE(gloss_zh, ?),
                    confidence = 0.8,
                    source = 'openai_backfill'
                WHERE id = ?
            """, (
                data.get('gender'),
                data.get('gloss_en'),
                data.get('gloss_zh'),
                sense_id
            ))
            
            # æ’å…¥æˆ–æ›´æ–° noun_props
            noun_props = data.get('noun_props', {})
            if noun_props:
                cursor.execute("""
                    INSERT OR REPLACE INTO noun_props 
                    (sense_id, gen_sg, plural, declension_class, dative_plural_ends_n)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    sense_id,
                    noun_props.get('gen_sg'),
                    noun_props.get('plural'),
                    noun_props.get('declension_class'),
                    noun_props.get('dative_plural_ends_n', False)
                ))
            
            # æ’å…¥è¯å½¢å˜åŒ–
            forms = data.get('forms', [])
            for form_data in forms:
                cursor.execute("""
                    INSERT OR IGNORE INTO forms_unimorph
                    (sense_id, form, features_json)
                    VALUES (?, ?, ?)
                """, (
                    sense_id,
                    form_data.get('form'),
                    json.dumps(form_data.get('features', {}))
                ))
            
            # æ’å…¥ä¾‹å¥ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            example = data.get('example')
            if example:
                cursor.execute("""
                    INSERT OR IGNORE INTO examples
                    (lemma_id, sense_id, de_text, en_text, zh_text, level)
                    SELECT ls.lemma_id, ?, ?, ?, ?, 'A1'
                    FROM lemma_senses ls 
                    WHERE ls.id = ?
                    AND NOT EXISTS (
                        SELECT 1 FROM examples e2 
                        WHERE e2.sense_id = ? AND e2.de_text = ?
                    )
                """, (
                    sense_id,
                    example.get('de'),
                    example.get('en'),
                    example.get('zh'),
                    sense_id,
                    sense_id,
                    example.get('de')
                ))
            
            conn.commit()
            self.stats['enriched_nouns'] += 1
            print(f"   âœ… åè¯æ•°æ®å·²ä¿å­˜")
            return True
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜åè¯æ•°æ®å¤±è´¥: {e}")
            conn.rollback()
            return False
        finally:
            conn.close()
    
    async def _save_verb_data(self, sense_id: int, data: Dict[str, Any]) -> bool:
        """ä¿å­˜åŠ¨è¯æ•°æ®åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æ›´æ–° lemma_senses
            cursor.execute("""
                UPDATE lemma_senses 
                SET gloss_en = COALESCE(gloss_en, ?),
                    gloss_zh = COALESCE(gloss_zh, ?),
                    confidence = 0.8,
                    source = 'openai_backfill'
                WHERE id = ?
            """, (
                data.get('gloss_en'),
                data.get('gloss_zh'),
                sense_id
            ))
            
            # æ’å…¥æˆ–æ›´æ–° verb_props
            cursor.execute("""
                INSERT OR REPLACE INTO verb_props 
                (sense_id, separable, prefix, aux, regularity, partizip_ii, reflexive, valency_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                sense_id,
                data.get('separable', False),
                data.get('prefix'),
                data.get('aux'),
                data.get('regularity'),
                data.get('partizip_ii'),
                data.get('reflexive', False),
                json.dumps(data.get('valency', {}))
            ))
            
            # æ’å…¥åŠ¨è¯å˜ä½å½¢å¼
            tables = data.get('tables', {})
            
            # PrÃ¤sens
            praesens = tables.get('praesens', {})
            for person, form in praesens.items():
                if form:
                    cursor.execute("""
                        INSERT OR IGNORE INTO forms_unimorph
                        (sense_id, form, features_json)
                        VALUES (?, ?, ?)
                    """, (
                        sense_id,
                        form,
                        json.dumps({"POS": "VERB", "Tense": "Pres", "Person": person})
                    ))
            
            # PrÃ¤teritum
            praeteritum = tables.get('praeteritum', {})
            for person, form in praeteritum.items():
                if form:
                    cursor.execute("""
                        INSERT OR IGNORE INTO forms_unimorph
                        (sense_id, form, features_json)
                        VALUES (?, ?, ?)
                    """, (
                        sense_id,
                        form,
                        json.dumps({"POS": "VERB", "Tense": "Past", "Person": person})
                    ))
            
            # æ’å…¥ä¾‹å¥
            example = data.get('example')
            if example:
                cursor.execute("""
                    INSERT OR IGNORE INTO examples
                    (lemma_id, sense_id, de_text, en_text, zh_text, level)
                    SELECT ls.lemma_id, ?, ?, ?, ?, 'A1'
                    FROM lemma_senses ls 
                    WHERE ls.id = ?
                    AND NOT EXISTS (
                        SELECT 1 FROM examples e2 
                        WHERE e2.sense_id = ? AND e2.de_text = ?
                    )
                """, (
                    sense_id,
                    example.get('de'),
                    example.get('en'),
                    example.get('zh'),
                    sense_id,
                    sense_id,
                    example.get('de')
                ))
            
            conn.commit()
            self.stats['enriched_verbs'] += 1
            print(f"   âœ… åŠ¨è¯æ•°æ®å·²ä¿å­˜")
            return True
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜åŠ¨è¯æ•°æ®å¤±è´¥: {e}")
            conn.rollback()
            return False
        finally:
            conn.close()
    
    async def _save_adjective_data(self, sense_id: int, data: Dict[str, Any]) -> bool:
        """ä¿å­˜å½¢å®¹è¯æ•°æ®åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æ›´æ–°åŸºç¡€ä¿¡æ¯
            cursor.execute("""
                UPDATE lemma_senses 
                SET gloss_en = COALESCE(gloss_en, ?),
                    gloss_zh = COALESCE(gloss_zh, ?),
                    confidence = 0.8,
                    source = 'openai_backfill'
                WHERE id = ?
            """, (
                data.get('gloss_en'),
                data.get('gloss_zh'),
                sense_id
            ))
            
            # æ’å…¥ä¾‹å¥
            example = data.get('example')
            if example:
                cursor.execute("""
                    INSERT OR IGNORE INTO examples
                    (lemma_id, sense_id, de_text, en_text, zh_text, level)
                    SELECT ls.lemma_id, ?, ?, ?, ?, 'A1'
                    FROM lemma_senses ls 
                    WHERE ls.id = ?
                """, (
                    sense_id,
                    example.get('de'),
                    example.get('en'),
                    example.get('zh'),
                    sense_id
                ))
            
            conn.commit()
            self.stats['enriched_adjectives'] += 1
            print(f"   âœ… å½¢å®¹è¯æ•°æ®å·²ä¿å­˜")
            return True
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜å½¢å®¹è¯æ•°æ®å¤±è´¥: {e}")
            conn.rollback()
            return False
        finally:
            conn.close()
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        elapsed = datetime.now() - self.stats['start_time']
        
        print(f"\nğŸ“Š å›å¡«ç»Ÿè®¡:")
        print(f"   å¤„ç†è¯æ¡: {self.stats['processed']}")
        print(f"   è¡¥å…¨åè¯: {self.stats['enriched_nouns']}")
        print(f"   è¡¥å…¨åŠ¨è¯: {self.stats['enriched_verbs']}")
        print(f"   è¡¥å…¨å½¢å®¹è¯: {self.stats['enriched_adjectives']}")
        print(f"   è·³è¿‡: {self.stats['skipped']}")
        print(f"   å¤±è´¥: {self.stats['failed']}")
        print(f"   ç”¨æ—¶: {elapsed}")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¾·è¯­è¯åº“æ‰¹é‡å›å¡«')
    parser.add_argument('--limit', type=int, default=50, help='å¤„ç†è¯æ¡æ•°é‡é™åˆ¶')
    parser.add_argument('--offset', type=int, default=0, help='å¼€å§‹åç§»é‡')
    parser.add_argument('--dry-run', action='store_true', help='è¯•è¿è¡Œï¼Œä¸ä¿å­˜æ•°æ®')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¾·è¯­è¯åº“æ‰¹é‡å›å¡«")
    print("=" * 50)
    
    if args.dry_run:
        print("âš ï¸ è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šä¿å­˜æ•°æ®")
    
    service = LexiconBackfillService()
    
    # è·å–éœ€è¦è¡¥å…¨çš„è¯æ¡
    incomplete_lemmas = service.get_incomplete_lemmas(args.limit, args.offset)
    
    if not incomplete_lemmas:
        print("âœ… æ‰€æœ‰è¯æ¡éƒ½å·²å®Œæ•´ï¼Œæ— éœ€å›å¡«")
        return
    
    print(f"å¼€å§‹å¤„ç† {len(incomplete_lemmas)} ä¸ªè¯æ¡...")
    
    # æ‰¹é‡å¤„ç†
    for i, lemma_data in enumerate(incomplete_lemmas, 1):
        print(f"\n[{i}/{len(incomplete_lemmas)}]", end=" ")
        
        if not args.dry_run:
            await service.enrich_lemma(lemma_data)
        else:
            print(f"è¯•è¿è¡Œ: {lemma_data['lemma']} ({lemma_data['upos']})")
        
        # æ¯10ä¸ªè¯æ¡ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…APIé™åˆ¶
        if i % 10 == 0:
            print(f"\nğŸ’¤ å·²å¤„ç† {i} ä¸ªè¯æ¡ï¼Œä¼‘æ¯3ç§’...")
            await asyncio.sleep(3)
    
    service.print_stats()
    print("\nğŸ‰ æ‰¹é‡å›å¡«å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
