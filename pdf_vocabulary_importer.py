#!/usr/bin/env python3
"""
PDFè¯æ±‡å¯¼å…¥å™¨ - ä»DTZè¯æ±‡åˆ—è¡¨PDFä¸­æå–å¾·è¯­å•è¯å¹¶å¯¼å…¥æ•°æ®åº“
æ”¯æŒè‡ªåŠ¨å»é‡ã€è¯æ€§è¯†åˆ«å’ŒLLMå¢å¼º
"""
import asyncio
import sys
import os
import re
import json
import sqlite3
from datetime import datetime
from typing import List, Set, Dict, Any
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(".env")
sys.path.append(os.getcwd())

try:
    import PyPDF2
except ImportError:
    print("âŒ éœ€è¦å®‰è£…PyPDF2åº“: pip install PyPDF2")
    sys.exit(1)

from app.services.lexicon_llm_service import LexiconLLMService

class PDFVocabularyExtractor:
    """PDFè¯æ±‡æå–å™¨ - ä½¿ç”¨AIè¿›è¡Œæ™ºèƒ½è¯æ±‡è¯†åˆ«"""
    
    def __init__(self):
        self.llm_service = LexiconLLMService()
        # åŸºæœ¬æ–‡æœ¬æ¸…ç†æ¨¡å¼
        self.cleanup_patterns = [
            r'\s+',  # å¤šä¸ªç©ºæ ¼
            r'[\x00-\x1f\x7f-\x9f]',  # æ§åˆ¶å­—ç¬¦
        ]
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                
                print(f"ğŸ“– PDFå…±æœ‰ {len(pdf_reader.pages)} é¡µ")
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        text += page_text + "\n"
                        
                        if page_num % 10 == 0:
                            print(f"   å·²å¤„ç† {page_num + 1} é¡µ...")
                            
                    except Exception as e:
                        print(f"   âš ï¸ ç¬¬ {page_num + 1} é¡µæå–å¤±è´¥: {e}")
                        continue
                
                return text
                
        except Exception as e:
            print(f"âŒ è¯»å–PDFå¤±è´¥: {e}")
            return ""
    
    async def extract_german_words_with_ai(self, text: str, chunk_size: int = 2000) -> Set[str]:
        """ä½¿ç”¨AIä»æ–‡æœ¬ä¸­æ™ºèƒ½æå–å¾·è¯­å•è¯"""
        words = set()
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self._clean_text(text)
        
        # å°†æ–‡æœ¬åˆ†å—å¤„ç†ï¼Œé¿å…è¶…è¿‡AIæ¨¡å‹é™åˆ¶
        text_chunks = self._split_text_into_chunks(cleaned_text, chunk_size)
        
        print(f"ğŸ“ æ–‡æœ¬åˆ†ä¸º {len(text_chunks)} ä¸ªå—è¿›è¡ŒAIåˆ†æ...")
        
        for i, chunk in enumerate(text_chunks):
            print(f"   å¤„ç†å— {i + 1}/{len(text_chunks)}...")
            
            try:
                chunk_words = await self._extract_words_from_chunk(chunk)
                words.update(chunk_words)
                
                # é¿å…APIé€Ÿç‡é™åˆ¶
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"   âš ï¸ å— {i + 1} å¤„ç†å¤±è´¥: {e}")
                continue
        
        return words
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ä¸å¿…è¦çš„å­—ç¬¦"""
        cleaned = text
        
        for pattern in self.cleanup_patterns:
            cleaned = re.sub(pattern, ' ', cleaned)
        
        return cleaned.strip()
    
    def _split_text_into_chunks(self, text: str, chunk_size: int) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆé€‚åˆAIå¤„ç†çš„å—"""
        chunks = []
        words = text.split()
        
        current_chunk = []
        current_length = 0
        
        for word in words:
            word_length = len(word) + 1  # +1 for space
            
            if current_length + word_length > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_length = word_length
            else:
                current_chunk.append(word)
                current_length += word_length
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    async def _extract_words_from_chunk(self, text_chunk: str) -> Set[str]:
        """ä½¿ç”¨AIä»æ–‡æœ¬å—ä¸­æå–å¾·è¯­è¯æ±‡"""
        system_prompt = """You are a German vocabulary extraction expert. Your task is to identify and extract all German vocabulary words (lemmas) from the given text.

Instructions:
1. Extract ONLY German vocabulary words (nouns, verbs, adjectives, adverbs, etc.)
2. Return the base form (lemma) of each word
3. For nouns: return without articles (e.g., "Haus" not "das Haus")
4. For verbs: return infinitive form (e.g., "gehen" not "geht")
5. For adjectives: return base form (e.g., "schÃ¶n" not "schÃ¶ne")
6. Exclude: numbers, punctuation, common function words (articles, prepositions, conjunctions)
7. Include compound words and technical terms
8. Return ONLY a JSON object with a "words" array, no explanation

Example output format:
{"words": ["Haus", "gehen", "schÃ¶n", "Deutschland", "arbeiten"]}"""

        user_prompt = f"""Extract all German vocabulary words from this text:

{text_chunk}

Return only a JSON object with a "words" array containing German lemmas."""

        try:
            # ä½¿ç”¨OpenAIæœåŠ¡
            response = await self.llm_service.openai_service.client.chat.completions.create(
                model=self.llm_service.openai_service.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=1000,
                response_format={"type": "json_object"}
            )
            
            # è§£æJSONå“åº”
            response_content = response.choices[0].message.content
            response_data = json.loads(response_content.strip())
            
            words_list = response_data.get('words', [])
            if isinstance(words_list, list):
                # è¿‡æ»¤å’ŒéªŒè¯å•è¯
                valid_words = set()
                for word in words_list:
                    if isinstance(word, str) and self._is_valid_german_word(word):
                        valid_words.add(word.strip())
                
                return valid_words
            else:
                print(f"   âš ï¸ AIè¿”å›æ ¼å¼ä¸æ­£ç¡®: {response_content[:100]}...")
                return set()
                
        except json.JSONDecodeError as e:
            print(f"   âš ï¸ JSONè§£æå¤±è´¥: {e}")
            return set()
        except Exception as e:
            print(f"   âš ï¸ AIæå–å¤±è´¥: {e}")
            return set()
    
    def _is_valid_german_word(self, word: str) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¾·è¯­å•è¯"""
        if not word or len(word) < 2 or len(word) > 30:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¾·è¯­å­—ç¬¦
        if not re.match(r'^[A-Za-zÃ„Ã–ÃœÃ¤Ã¶Ã¼ÃŸ]+$', word):
            return False
        
        # æ’é™¤çº¯æ•°å­—æˆ–è¿‡çŸ­çš„å•è¯
        if word.isdigit() or (word.isupper() and len(word) <= 3):
            return False
        
        return True
    
    async def categorize_words_with_ai(self, words: Set[str], batch_size: int = 50) -> Dict[str, List[str]]:
        """ä½¿ç”¨AIå¯¹è¯æ±‡è¿›è¡Œåˆ†ç±»"""
        categorized = {
            'nouns': [],
            'verbs': [],
            'adjectives': [],
            'others': []
        }
        
        word_list = list(words)
        total_batches = (len(word_list) + batch_size - 1) // batch_size
        
        print(f"ğŸ·ï¸ ä½¿ç”¨AIå¯¹ {len(word_list)} ä¸ªå•è¯è¿›è¡Œè¯æ€§åˆ†ç±»...")
        print(f"   åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†")
        
        for i in range(0, len(word_list), batch_size):
            batch = word_list[i:i + batch_size]
            print(f"   å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches}...")
            
            try:
                batch_categorized = await self._categorize_word_batch(batch)
                
                # åˆå¹¶ç»“æœ
                for category, batch_words in batch_categorized.items():
                    if category in categorized:
                        categorized[category].extend(batch_words)
                
                # é¿å…APIé€Ÿç‡é™åˆ¶
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"   âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} åˆ†ç±»å¤±è´¥: {e}")
                # å¦‚æœAIåˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•
                for word in batch:
                    if word[0].isupper():
                        categorized['nouns'].append(word)
                    elif word.lower().endswith('en'):
                        categorized['verbs'].append(word)
                    else:
                        categorized['others'].append(word)
        
        return categorized
    
    async def _categorize_word_batch(self, words: List[str]) -> Dict[str, List[str]]:
        """ä½¿ç”¨AIå¯¹ä¸€æ‰¹å•è¯è¿›è¡Œè¯æ€§åˆ†ç±»"""
        system_prompt = """You are a German linguistic expert. Classify the given German words by their part of speech (POS).

Instructions:
1. Classify each word as: NOUN, VERB, ADJECTIVE, or OTHER
2. For nouns: include compound nouns, proper nouns
3. For verbs: include infinitive forms, separable verbs
4. For adjectives: include comparative/superlative forms
5. OTHER: adverbs, prepositions, conjunctions, particles, etc.
6. Return ONLY a JSON object with words categorized by POS

Example output format:
{
  "nouns": ["Haus", "Deutschland", "Arbeit"],
  "verbs": ["gehen", "arbeiten", "verstehen"],
  "adjectives": ["schÃ¶n", "groÃŸ", "wichtig"],
  "others": ["schnell", "aber", "mit"]
}"""

        words_text = ", ".join(words)
        user_prompt = f"""Classify these German words by part of speech:

{words_text}

Return only the JSON categorization."""

        try:
            response = await self.llm_service.openai_service.client.chat.completions.create(
                model=self.llm_service.openai_service.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=800,
                response_format={"type": "json_object"}
            )
            
            response_content = response.choices[0].message.content
            categorization = json.loads(response_content.strip())
            
            # æ ‡å‡†åŒ–é”®å
            normalized = {}
            for key, word_list in categorization.items():
                normalized_key = key.lower().rstrip('s')  # "nouns" -> "noun"
                if normalized_key in ['noun', 'verb', 'adjective']:
                    normalized[normalized_key + 's'] = word_list
                else:
                    normalized['others'] = normalized.get('others', []) + word_list
            
            return normalized
            
        except Exception as e:
            print(f"   âš ï¸ AIåˆ†ç±»å¤±è´¥: {e}")
            return {"nouns": [], "verbs": [], "adjectives": [], "others": words}

class PDFDatabaseImporter:
    """PDFè¯æ±‡æ•°æ®åº“å¯¼å…¥å™¨"""
    
    def __init__(self):
        self.llm_service = LexiconLLMService()
        self.db_path = 'data/app.db'
        self.stats = {
            'total_words': 0,
            'existing_words': 0,
            'new_words': 0,
            'enhanced_words': 0,
            'failed_words': 0,
            'start_time': datetime.now()
        }
    
    def word_exists_in_database(self, lemma: str) -> bool:
        """æ£€æŸ¥å•è¯æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æ£€æŸ¥æ–°æ¶æ„ï¼ˆlemma_sensesè¡¨ï¼‰
            cursor.execute("""
                SELECT COUNT(*) FROM word_lemmas 
                WHERE LOWER(lemma) = LOWER(?)
            """, (lemma,))
            
            new_count = cursor.fetchone()[0]
            if new_count > 0:
                return True
            
            # æ£€æŸ¥æ—§æ¶æ„ï¼ˆå¦‚æœå­˜åœ¨translationsè¡¨ï¼‰
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='translations'
            """)
            
            if cursor.fetchone():
                cursor.execute("""
                    SELECT COUNT(*) FROM word_lemmas wl
                    WHERE LOWER(wl.lemma) = LOWER(?)
                """, (lemma,))
                
                old_count = cursor.fetchone()[0]
                return old_count > 0
            
            return False
            
        finally:
            conn.close()
    
    async def insert_word_to_database(self, lemma: str, estimated_pos: str = None) -> bool:
        """å°†å•è¯æ’å…¥æ•°æ®åº“ï¼Œä½¿ç”¨LLMè¿›è¡Œåˆ†æ"""
        try:
            print(f"ğŸ” åˆ†æå•è¯: {lemma}")
            
            # ä½¿ç”¨LLMæœåŠ¡è¿›è¡Œè¯æ±‡æ¶ˆæ­§
            disambiguation = await self.llm_service.disambiguate_lemma(lemma)
            
            if not disambiguation or not disambiguation.get('senses'):
                print(f"   âŒ æ— æ³•åˆ†æè¯æ±‡: {lemma}")
                self.stats['failed_words'] += 1
                return False
            
            # å–ç¬¬ä¸€ä¸ªï¼ˆæœ€å¯èƒ½çš„ï¼‰è¯æ€§
            primary_sense = disambiguation['senses'][0]
            upos = primary_sense.get('upos', 'OTHER')
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            try:
                # æ’å…¥word_lemmas
                cursor.execute("""
                    INSERT INTO word_lemmas (lemma, pos, cefr, notes, created_at)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    lemma,
                    upos.lower(),
                    'A1',  # é»˜è®¤çº§åˆ«
                    f'Imported from DTZ PDF - estimated: {estimated_pos or "unknown"}',
                    datetime.now().isoformat()
                ))
                
                lemma_id = cursor.lastrowid
                
                # æ’å…¥lemma_senses
                cursor.execute("""
                    INSERT INTO lemma_senses 
                    (lemma_id, upos, xpos, gender, gloss_en, gloss_zh, confidence, source)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    lemma_id,
                    upos,
                    primary_sense.get('xpos', 'OTHER'),
                    primary_sense.get('gender'),
                    primary_sense.get('gloss_en'),
                    primary_sense.get('gloss_zh'),
                    0.8,  # ä¸­ç­‰ç½®ä¿¡åº¦
                    'pdf_dtz_import'
                ))
                
                sense_id = cursor.lastrowid
                
                # æ ¹æ®è¯æ€§ä¿å­˜ç‰¹å®šå±æ€§
                if upos == 'NOUN':
                    await self._save_noun_enhanced_data(cursor, sense_id, lemma)
                elif upos == 'VERB':
                    await self._save_verb_enhanced_data(cursor, sense_id, lemma)
                
                conn.commit()
                print(f"   âœ… æˆåŠŸå¯¼å…¥: {lemma} ({upos})")
                self.stats['new_words'] += 1
                return True
                
            finally:
                conn.close()
                
        except Exception as e:
            print(f"   âŒ å¯¼å…¥å¤±è´¥: {lemma} - {e}")
            self.stats['failed_words'] += 1
            return False
    
    async def _save_noun_enhanced_data(self, cursor, sense_id: int, lemma: str):
        """ä¿å­˜åè¯çš„å¢å¼ºæ•°æ®"""
        try:
            noun_data = await self.llm_service.enrich_noun(lemma)
            if noun_data and noun_data.get('noun_props'):
                props = noun_data['noun_props']
                cursor.execute("""
                    INSERT INTO noun_props 
                    (sense_id, gen_sg, plural, declension_class, dative_plural_ends_n)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    sense_id,
                    props.get('gen_sg'),
                    props.get('plural'),
                    props.get('declension_class'),
                    props.get('dative_plural_ends_n', False)
                ))
                self.stats['enhanced_words'] += 1
        except Exception as e:
            print(f"     âš ï¸ åè¯å¢å¼ºå¤±è´¥: {e}")
    
    async def _save_verb_enhanced_data(self, cursor, sense_id: int, lemma: str):
        """ä¿å­˜åŠ¨è¯çš„å¢å¼ºæ•°æ®"""
        try:
            verb_data = await self.llm_service.enrich_verb(lemma)
            if verb_data:
                cursor.execute("""
                    INSERT INTO verb_props 
                    (sense_id, separable, prefix, aux, regularity, partizip_ii, reflexive, valency_json)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    sense_id,
                    verb_data.get('separable', False),
                    verb_data.get('prefix'),
                    verb_data.get('aux'),
                    verb_data.get('regularity'),
                    verb_data.get('partizip_ii'),
                    verb_data.get('reflexive', False),
                    '{}' if not verb_data.get('valency') else str(verb_data.get('valency'))
                ))
                self.stats['enhanced_words'] += 1
        except Exception as e:
            print(f"     âš ï¸ åŠ¨è¯å¢å¼ºå¤±è´¥: {e}")
    
    async def batch_import_words(self, words: List[str], estimated_pos: str = None, batch_size: int = 10):
        """æ‰¹é‡å¯¼å…¥å•è¯"""
        total = len(words)
        print(f"ğŸ“š å¼€å§‹æ‰¹é‡å¯¼å…¥ {total} ä¸ª{estimated_pos or ''}å•è¯...")
        
        for i in range(0, total, batch_size):
            batch = words[i:i + batch_size]
            print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1} ({len(batch)} ä¸ªå•è¯)")
            
            for word in batch:
                self.stats['total_words'] += 1
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if self.word_exists_in_database(word):
                    print(f"â© '{word}' å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    self.stats['existing_words'] += 1
                    continue
                
                # æ’å…¥æ–°å•è¯
                await self.insert_word_to_database(word, estimated_pos)
                
                # ç¨ä½œå»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                await asyncio.sleep(0.5)
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if i + batch_size < total:
                print("ğŸ’¤ æ‰¹æ¬¡å®Œæˆï¼Œä¼‘æ¯2ç§’...")
                await asyncio.sleep(2)
        
        self._print_final_stats()
    
    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        elapsed = datetime.now() - self.stats['start_time']
        
        print(f"\nğŸ‰ PDFè¯æ±‡å¯¼å…¥å®Œæˆï¼")
        print(f"=" * 50)
        print(f"æ€»å¤„ç†å•è¯: {self.stats['total_words']}")
        print(f"å·²å­˜åœ¨: {self.stats['existing_words']}")
        print(f"æ–°å¯¼å…¥: {self.stats['new_words']}")
        print(f"å¢å¼ºå¤„ç†: {self.stats['enhanced_words']}")
        print(f"å¤±è´¥: {self.stats['failed_words']}")
        print(f"æ€»ç”¨æ—¶: {elapsed}")

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä»DTZ PDFè¯æ±‡åˆ—è¡¨å¯¼å…¥å¾·è¯­å•è¯åˆ°æ•°æ®åº“')
    parser.add_argument('pdf_file', help='DTZ PDFè¯æ±‡åˆ—è¡¨æ–‡ä»¶')
    parser.add_argument('--batch-size', type=int, default=10, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--dry-run', action='store_true', help='è¯•è¿è¡Œæ¨¡å¼ï¼ˆåªæå–ï¼Œä¸å¯¼å…¥ï¼‰')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†çš„å•è¯æ•°é‡')
    parser.add_argument('--category', choices=['nouns', 'verbs', 'adjectives', 'all'], 
                       default='all', help='å¯¼å…¥çš„è¯æ±‡ç±»åˆ«')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.pdf_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.pdf_file}")
        return
    
    print("ğŸš€ DTZ PDFè¯æ±‡å¯¼å…¥å·¥å…·")
    print("=" * 50)
    
    # 1. æå–PDFæ–‡æœ¬
    print("ğŸ“– æå–PDFæ–‡æœ¬...")
    extractor = PDFVocabularyExtractor()
    text = extractor.extract_text_from_pdf(args.pdf_file)
    
    if not text:
        print("âŒ æ— æ³•æå–PDFæ–‡æœ¬")
        return
    
    # 2. ä½¿ç”¨AIæå–å¾·è¯­å•è¯
    print("ğŸ¤– ä½¿ç”¨AIæ™ºèƒ½æå–å¾·è¯­å•è¯...")
    words = await extractor.extract_german_words_with_ai(text)
    print(f"ğŸ“Š AIæå–åˆ° {len(words)} ä¸ªå¾·è¯­å•è¯")
    
    # 3. ä½¿ç”¨AIåˆ†ç±»å•è¯
    categorized = await extractor.categorize_words_with_ai(words)
    
    print("\nğŸ“‹ è¯æ±‡åˆ†ç±»ç»Ÿè®¡:")
    for category, word_list in categorized.items():
        print(f"   {category}: {len(word_list)} ä¸ª")
    
    if args.dry_run:
        print("\nâš ï¸ è¯•è¿è¡Œæ¨¡å¼ - æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ:")
        for category, word_list in categorized.items():
            if word_list:
                print(f"\n{category} (å‰10ä¸ª):")
                for word in sorted(word_list)[:10]:
                    print(f"   - {word}")
        return
    
    # 4. å¯¼å…¥æ•°æ®åº“
    importer = PDFDatabaseImporter()
    
    # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ç±»åˆ«è¿›è¡Œå¯¼å…¥
    if args.category == 'all':
        for category, word_list in categorized.items():
            if word_list:
                limited_words = word_list[:args.limit] if args.limit else word_list
                await importer.batch_import_words(
                    sorted(limited_words), 
                    category.rstrip('s'),  # "nouns" -> "noun"
                    args.batch_size
                )
    else:
        selected_category = args.category
        if selected_category in categorized and categorized[selected_category]:
            word_list = categorized[selected_category]
            limited_words = word_list[:args.limit] if args.limit else word_list
            await importer.batch_import_words(
                sorted(limited_words),
                selected_category.rstrip('s'),  # "nouns" -> "noun"
                args.batch_size
            )

if __name__ == "__main__":
    asyncio.run(main())
